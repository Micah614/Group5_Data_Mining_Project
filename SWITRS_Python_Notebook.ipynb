{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bed2bbc5-9d29-4c0b-812e-9fcbe775a582",
   "metadata": {},
   "source": [
    "<h1> Python Data Notebook (Template) </h1>\n",
    "<h3> Preserve this template, duplicate this file before using it!!</h3>\n",
    "<strong> Author: Micah Simmerman, Nolan Ollada, Nathan Palmer </strong>\n",
    "\n",
    "<strong>Resource URL:</strong>\n",
    "\n",
    "<strong>Database file(s), columnar dataset(s), etc.:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de39b907-6616-4c1f-8c75-8a020d020783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 tables found.\n",
      "Table and attribute name extraction completed successfully.\n",
      "DB file and Dependencies Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Libraries and packages\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "pd.__version__\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# SQLite db file connection is achieved in the try/except statements below. \n",
    "# TODO: add sections to read-in mySQL/PostGreSQL and csv files.\n",
    "\n",
    "tables = []  # GLOBAL TABLE INDEX\n",
    "attribute_list = []  # GLOBAL ATTRIBUTE INDEX\n",
    "\n",
    "# NOTE: always store the switrs.sqlite file in the parent directory where this python notebook is kept.\n",
    "db_file = '../switrs.sqlite'  # name the db file downloaded directly from https://www.kaggle.com/datasets/alexgude/california-traffic-collision-data-from-switrs\n",
    "sqliteConnection = sqlite3.connect(db_file)\n",
    "\n",
    "# if os.path.isfile(db_file):  # determine if there is any SQLite db file by that name exists in the specified file location.\n",
    "sqliteConnection = sqlite3.connect(db_file)  # establish a connection if the file does exist.\n",
    "cursor = sqliteConnection.cursor()  # create a cursor object.\n",
    "table_query = 'SELECT name from sqlite_master where type= \"table\"'\n",
    "cursor.execute(table_query)\n",
    "result = cursor.fetchall()\n",
    "print()\n",
    "print(len(result), \"tables found.\")\n",
    "for i in range(len(result)):\n",
    "    tables.append(result[i][0])\n",
    "    # print(result[i][0])\n",
    "\n",
    "# the second part of the algorithm extracts the attributes of each table identified in the first step and places them into a 2D list. \n",
    "for table in tables:\n",
    "    consumer_complaints_count_records = \"PRAGMA table_info(\"\n",
    "    consumer_complaints_count_records += str(table)  + \");\"  # build the sqlite query string using the current list object\n",
    "    cursor.execute(consumer_complaints_count_records)  # execute the query string\n",
    "    result = cursor.fetchall()  # collect the results\n",
    "    temp_list = []\n",
    "    for item in result:\n",
    "        temp_list.append(item[1])\n",
    "    attribute_list.append(temp_list)  # export the list attribute table after it is built\n",
    "    # print(\"The\", table, \"table contains\", result[0][0], \"data points\")  # print the results\n",
    "    # print()\n",
    "print(\"Table and attribute name extraction completed successfully.\")\n",
    "\n",
    "\n",
    "print('DB file and Dependencies Loaded.')  \n",
    "# else:\n",
    "#     print(os.path.isfile(db_file), \"No SQLite file detected.\")  # otherwise, print the connection status."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "327c63e2",
   "metadata": {},
   "source": [
    "Start by collecting information about the table names in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec84088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:  ['case_ids', 'collisions', 'victims', 'parties']\n",
      "Attribute list:  [['case_id', 'db_year'], ['case_id', 'jurisdiction', 'officer_id', 'reporting_district', 'chp_shift', 'population', 'county_city_location', 'county_location', 'special_condition', 'beat_type', 'chp_beat_type', 'city_division_lapd', 'chp_beat_class', 'beat_number', 'primary_road', 'secondary_road', 'distance', 'direction', 'intersection', 'weather_1', 'weather_2', 'state_highway_indicator', 'caltrans_county', 'caltrans_district', 'state_route', 'route_suffix', 'postmile_prefix', 'postmile', 'location_type', 'ramp_intersection', 'side_of_highway', 'tow_away', 'collision_severity', 'killed_victims', 'injured_victims', 'party_count', 'primary_collision_factor', 'pcf_violation_code', 'pcf_violation_category', 'pcf_violation', 'pcf_violation_subsection', 'hit_and_run', 'type_of_collision', 'motor_vehicle_involved_with', 'pedestrian_action', 'road_surface', 'road_condition_1', 'road_condition_2', 'lighting', 'control_device', 'chp_road_type', 'pedestrian_collision', 'bicycle_collision', 'motorcycle_collision', 'truck_collision', 'not_private_property', 'alcohol_involved', 'statewide_vehicle_type_at_fault', 'chp_vehicle_type_at_fault', 'severe_injury_count', 'other_visible_injury_count', 'complaint_of_pain_injury_count', 'pedestrian_killed_count', 'pedestrian_injured_count', 'bicyclist_killed_count', 'bicyclist_injured_count', 'motorcyclist_killed_count', 'motorcyclist_injured_count', 'primary_ramp', 'secondary_ramp', 'latitude', 'longitude', 'collision_date', 'collision_time', 'process_date'], ['id', 'case_id', 'party_number', 'victim_role', 'victim_sex', 'victim_age', 'victim_degree_of_injury', 'victim_seating_position', 'victim_safety_equipment_1', 'victim_safety_equipment_2', 'victim_ejected'], ['id', 'case_id', 'party_number', 'party_type', 'at_fault', 'party_sex', 'party_age', 'party_sobriety', 'party_drug_physical', 'direction_of_travel', 'party_safety_equipment_1', 'party_safety_equipment_2', 'financial_responsibility', 'hazardous_materials', 'cellphone_in_use', 'cellphone_use_type', 'school_bus_related', 'oaf_violation_code', 'oaf_violation_category', 'oaf_violation_section', 'oaf_violation_suffix', 'other_associate_factor_1', 'other_associate_factor_2', 'party_number_killed', 'party_number_injured', 'movement_preceding_collision', 'vehicle_year', 'vehicle_make', 'statewide_vehicle_type', 'chp_vehicle_type_towing', 'chp_vehicle_type_towed', 'party_race']]\n"
     ]
    }
   ],
   "source": [
    "# Now we have a list of tables and their respective attribute columns.\n",
    "print(\"Tables in the database: \", tables)\n",
    "print(\"Attribute list: \", attribute_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0dc1f9",
   "metadata": {},
   "source": [
    "### Database editing\n",
    "run the code below to remove the attributes in the remove_cols_collisions and remove_cols_parties tables specified. You can run the above 2 code blocks again to see that the specified attributes were removed.\n",
    "\n",
    "Running the following cell twice currently results in an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d6c78",
   "metadata": {},
   "source": [
    "    RATIONAL FOR ATTRIBUTE COLUMN REMOVAL:\n",
    "    collisions.officer_id: responding officers identification is not relevant to the factors of a collision.\n",
    "    collisions.chp_shift: chp_shift indicates the work shift period of the responding officer. More accurate temporal measurements exist.\n",
    "    collisions.special_condition: \n",
    "    collisions.beat_type: \n",
    "    collisions.city_division_lapd: \n",
    "    collisions.chp_beat_class: \n",
    "    collisions.beat_number: \n",
    "    collisions.pcf_violation: \n",
    "    collisions.pcf_violation_subsection: \n",
    "    collisions.hit_and_run: \n",
    "    collisions.process_date: \n",
    "\n",
    "    For a complete list of attribute definitions, please visit: https://peteraldhous.com/Data/ca_traffic/SWITRS_codebook.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77330f",
   "metadata": {},
   "source": [
    "# Warning:\n",
    "The following cell block removes unwanted attribute columns from the SWITRS database file and will take a long time to execute. Repeated requests should not impose harm on the database file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965fcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the page count to \n",
    "increase_page_count = \"PRAGMA max_page_count = 2147483646;\"\n",
    "turn_off_journal = \"PRAGMA journal_mode=OFF\"\n",
    "cursor.execute(increase_page_count)\n",
    "cursor.execute(turn_off_journal)  # turn off the journaling mode (temporarily) to avoid exceeding local disk space limitations.\n",
    "sqliteConnection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e10693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed officer_id from the 'collisions' table\n",
      "Removed chp_shift from the 'collisions' table\n"
     ]
    }
   ],
   "source": [
    "# Alternative code for the cell block below, attempting to minimize space the space requirements for data processing. \n",
    "\n",
    "\n",
    "# The columns you want to remove\n",
    "remove_cols_collisions = [\"officer_id\", \"chp_shift\", \"special_condition\", \"beat_type\", \"city_division_lapd\", \"chp_beat_class\", \"beat_number\", \"pcf_violation\", \"pcf_violation_subsection\", \"hit_and_run\", \"process_date\"]\n",
    "# remove_cols_collisions = \"officer_id, chp_shift, special_condition, beat_type, city_division_lapd, chp_beat_class, beat_number, pcf_violation, pcf_violation_subsection, hit_and_run, process_date\"\n",
    "remove_cols_parties = [\"hazardous_materials\", \"oaf_violation_code\"]\n",
    "# remove_cols_parties = \"hazardous_materials, oaf_violation_code\"\n",
    "\n",
    "# drop_collisions_columns = \"ALTER TABLE collisions DROP \"\n",
    "# drop_collisions_columns += remove_cols_collisions\n",
    "# drop_collisions_columns += \";\"\n",
    "# cursor.execute(drop_collisions_columns, remove_cols_collisions) \n",
    "\n",
    "# cursor.execute(\"ALTER TABLE collisions DROP officer_id;\")  # try one to start.\n",
    "# sqliteConnection.commit()\n",
    "    \n",
    "#     drop_collisions_columns)\n",
    "# sqliteConnection.commit()\n",
    "\n",
    "\n",
    "for col in remove_cols_collisions:\n",
    "    drop_collisions_columns = \"ALTER TABLE collisions DROP  \"\n",
    "    drop_collisions_columns += col  + \";\" \n",
    "    cursor.execute(drop_collisions_columns)\n",
    "    sqliteConnection.commit()\n",
    "    print(\"Removed\", col, \"from the 'collisions' table.\")\n",
    "    print(\"working...\")\n",
    "for attrib in remove_cols_parties:\n",
    "    drop_collisions_columns = \"ALTER TABLE parties DROP \"\n",
    "    drop_collisions_columns += attrib  + \";\" \n",
    "    cursor.execute(drop_collisions_columns)\n",
    "    sqliteConnection.commit()\n",
    "    print(\"Removed\", col, \"from the 'collisions' table\")\n",
    "    print(\"working...\")\n",
    "    \n",
    "turn_journal_on = \"PRAGMA journal_mode=ON\"\n",
    "cursor.execute(turn_journal_on)  # turn the journaling mode back on so we can roll-back our changes.\n",
    "sqliteConnection.commit()  # commit the change to the database file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_new_column_string(old_columns, columns_to_remove):\n",
    "#     new_columns = [col for col in old_columns if col not in columns_to_remove]\n",
    "#     return \", \".join(new_columns)\n",
    "\n",
    "# # The columns you want to remove\n",
    "# remove_cols_collisions = ['officer_id', 'chp_shift', 'special_condition', \n",
    "#                           'beat_type', 'city_division_lapd', 'chp_beat_class', \n",
    "#                           'beat_number', 'pcf_violation', 'pcf_violation_subsection', \n",
    "#                           'hit_and_run', 'process_date']\n",
    "# remove_cols_parties = ['hazardous_materials', 'oaf_violation_code']\n",
    "\n",
    "\n",
    "# # The new column strings\n",
    "# new_cols_collisions = get_new_column_string(attribute_list[1], remove_cols_collisions)\n",
    "# new_cols_parties = get_new_column_string(attribute_list[3], remove_cols_parties)\n",
    "\n",
    "# # Create new tables without the specified columns\n",
    "# cursor.execute(f'''\n",
    "#     CREATE TABLE new_collisions AS \n",
    "#     SELECT {new_cols_collisions} FROM collisions WHERE 1=0;\n",
    "# ''')\n",
    "\n",
    "# cursor.execute(f'''\n",
    "#     CREATE TABLE new_parties AS \n",
    "#     SELECT {new_cols_parties} FROM parties WHERE 1=0;\n",
    "# ''')\n",
    "\n",
    "# # Insert data from the old tables into the new ones\n",
    "# cursor.execute(f'''\n",
    "#     INSERT INTO new_collisions SELECT {new_cols_collisions} FROM collisions;\n",
    "# ''')\n",
    "\n",
    "# cursor.execute(f'''\n",
    "#     INSERT INTO new_parties SELECT {new_cols_parties} FROM parties;\n",
    "# ''')\n",
    "\n",
    "# # Delete the old tables\n",
    "# cursor.execute('DROP TABLE collisions;')\n",
    "# cursor.execute('DROP TABLE parties;')\n",
    "\n",
    "# # Rename the new tables to the original names\n",
    "# cursor.execute('ALTER TABLE new_collisions RENAME TO collisions;')\n",
    "# cursor.execute('ALTER TABLE new_parties RENAME TO parties;')\n",
    "\n",
    "# # Commit the changes and close the connection\n",
    "# sqliteConnection.commit()\n",
    "# # sqliteConnection.close()  # creates \"cannot operate on a closed database\" error in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb8b07",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT * FROM collisions\nJOIN parties ON collisions.case_id = parties.case_id\nJOIN victims ON parties.case_id = victims.case_id AND parties.party_number = victims.party_number\nORDER BY RANDOM() \nLIMIT 10\n': database or disk is full",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:2202\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2201\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2202\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(sql, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   2203\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: database or disk is full",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m random_entries_query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mSELECT * FROM collisions\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mJOIN parties ON collisions.case_id = parties.case_id\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mLIMIT 10\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m random_entries \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql_query(random_entries_query, sqliteConnection)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:469\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    466\u001b[0m     dtype_backend \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39mwith\u001b[39;00m pandasSQL_builder(con) \u001b[39mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 469\u001b[0m     \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[0;32m    470\u001b[0m         sql,\n\u001b[0;32m    471\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m    472\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    473\u001b[0m         coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[0;32m    474\u001b[0m         parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m    475\u001b[0m         chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    476\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    477\u001b[0m         dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m    478\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:2266\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_query\u001b[39m(\n\u001b[0;32m   2256\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2257\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2264\u001b[0m     dtype_backend: DtypeBackend \u001b[39m|\u001b[39m Literal[\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2265\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2266\u001b[0m     cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(sql, params)\n\u001b[0;32m   2267\u001b[0m     columns \u001b[39m=\u001b[39m [col_desc[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m col_desc \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mdescription]\n\u001b[0;32m   2269\u001b[0m     \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\sql.py:2214\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2211\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[0;32m   2213\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msql\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2214\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT * FROM collisions\nJOIN parties ON collisions.case_id = parties.case_id\nJOIN victims ON parties.case_id = victims.case_id AND parties.party_number = victims.party_number\nORDER BY RANDOM() \nLIMIT 10\n': database or disk is full"
     ]
    }
   ],
   "source": [
    "random_entries_query = \"\"\"\n",
    "SELECT * FROM collisions\n",
    "JOIN parties ON collisions.case_id = parties.case_id\n",
    "JOIN victims ON parties.case_id = victims.case_id AND parties.party_number = victims.party_number\n",
    "ORDER BY RANDOM() \n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "random_entries = pd.read_sql_query(random_entries_query, sqliteConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063affc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The case_ids table contains 9424334 data points\n",
      "The collisions table contains 9424334 data points\n",
      "The victims table contains 9639334 data points\n",
      "The parties table contains 18669166 data points\n"
     ]
    }
   ],
   "source": [
    "# This cell will determine the number of data points within each table of the database and enumerate them one by one.\n",
    "\n",
    "# attribute_list = []\n",
    "# count_records = 'SELECT COUNT(*) FROM ('  # start the string.\n",
    "# count_records += table + \");\"  # concatenate the variable and complete the string build.\n",
    "# cursor.execute(count_records)  # \n",
    "# result = cursor.fetchall()\n",
    "\n",
    "for table in range(len(tables)):\n",
    "    \n",
    "    count_records = 'SELECT COUNT(*) FROM ('  # start the string.\n",
    "    count_records += tables[table] + \");\"  # concatenate the variable and complete the string build.\n",
    "    cursor.execute(count_records)  # \n",
    "    result = cursor.fetchall()\n",
    "    print(\"The\", tables[table], \"table contains\", result[0][0], \"data points\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a5fc3c6",
   "metadata": {},
   "source": [
    "TO DO: INSERT BASIC STATISTICS FUNCTIONS BETWEEN THESE TWO CELLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will defines a first round Data Cube Function(s)\n",
    "create_data_cube = \"\"\"SELECT \n",
    "    c1, c2, AGGREGATE_FUNCTION(c3)\n",
    "FROM\n",
    "    table_name\n",
    "GROUP BY CUBE(c1 , c2);\"\"\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a666f304",
   "metadata": {},
   "source": [
    "The functions below currently only work on csv files. Add a conditional behavior to these functions and extend them to extract equivalent data from a table in an SQLite db file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are basic statistic functions that can be used as helper routines in longer extraction/cleaning/clustering efforts. \n",
    "# The remainder of the notebook contains code written to examine the telescope data. You can to generalize each function or create a\n",
    "# python scrip to examine tabular data files in a terminal window.\n",
    "\n",
    "# There are many methods that you can use to extract SQLite table data into a pandas dataframe or a numpy data object to generate graphics, \n",
    "# or perform package-based statistical testing. At this point in the notebook (and as of 11-June-2023).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# maximum value (max)\n",
    "def maximum(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)  # read the file into a pandas dataframe\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]  # project the indicated column from the pandas dataframe\n",
    "        result = column.max()  # find the maximum value\n",
    "        return round(result, 2)\n",
    "\n",
    "# minimum value (min)\n",
    "def minimum(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.min()  # find the minimum value\n",
    "        return round(result, 2)\n",
    "\n",
    "# average (mean)\n",
    "def mean(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.mean()  # calculate the average value\n",
    "        return round(result, 2)\n",
    "\n",
    "# standard deviation\n",
    "def std(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.std()  # calculate standard deviation\n",
    "        return round(result, 2)\n",
    "\n",
    "# first quantile, Q1\n",
    "def q1(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]\n",
    "        result = column.quantile([0.25]).values[0]\n",
    "        return round(result, 2)\n",
    "\n",
    "# third quantile, Q3\n",
    "def q3(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]\n",
    "        result = column.quantile([0.75]).values[0]\n",
    "        return round(result, 2)\n",
    "\n",
    "# median, Q2\n",
    "def median(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]  # slice the indiced column\n",
    "        result = column.quantile([0.5]).values[0]  # compute the second quantile\n",
    "        return round(result, 2)\n",
    "\n",
    "# inner-quartile range\n",
    "def iqr(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        iqr = float(q3(filename, attribute_number))-float(q1(filename, attribute_number))\n",
    "        # return \"{:.2f}\".format(iqr)  # format to 2 decimal places.\n",
    "        return round(iqr, 2)\n",
    "\n",
    "# N (number of objects)\n",
    "def count(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]  # slice the indicated column\n",
    "        result = column.count()  # count \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56661eb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# OUTPUT TESTING\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mN:\u001b[39m\u001b[39m\"\u001b[39m, count(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m7\u001b[39;49m))  \u001b[39m# N\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQ1:\u001b[39m\u001b[39m\"\u001b[39m, q1(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m7\u001b[39m))  \u001b[39m# Q1\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmedian:\u001b[39m\u001b[39m\"\u001b[39m, median(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m7\u001b[39m))  \u001b[39m# Median (Q2)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 98\u001b[0m, in \u001b[0;36mcount\u001b[1;34m(filename, attribute_number)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     MC_telescope_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(filename, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     99\u001b[0m     MC_telescope_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(MC_telescope_data)\n\u001b[0;32m    100\u001b[0m     column \u001b[39m=\u001b[39m MC_telescope_df\u001b[39m.\u001b[39miloc[:,attribute_number]  \u001b[39m# slice the indicated column\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# OUTPUT TESTING\n",
    "print(\"N:\", count(\"\", 7))  # N\n",
    "print(\"Q1:\", q1(\"\", 7))  # Q1\n",
    "print(\"median:\", median(\"\", 7))  # Median (Q2)\n",
    "print(\"Q3:\", q3(\"\", 7))  # Q3\n",
    "print(\"IQR:\", iqr(\"\", 7))  # IQR\n",
    "print(\"min:\", minimum(\"\", 7))  # min\n",
    "print(\"max:\", maximum(\"\", 7))  # max\n",
    "print(\"average:\", mean(\"\", 7))  # mean\n",
    "print(\"std. deviation:\", std(\"\", 7))  # standard deviation\n",
    "\n",
    "# # ERROR HANDLING\n",
    "# print(maximum(\"magic04.data\", 2.1))  # float (in-range)\n",
    "# print(minimum(\"magic04.data\", -1))  # out of bounds (less than)\n",
    "# print(mean(\"magic04.data\", 0))  # zero\n",
    "# print(std(\"magic04.data\", -1.5))  # negative out of range float\n",
    "# print(q1(\"magic04.data\", 10.5))  # positive out of range float\n",
    "# print(median(\"magic04.data\", 11))  # positive out of range (greater than)\n",
    "# print(q3(\"magic04.data\", -11.5))  # negative out of range float (less than)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b743b155",
   "metadata": {},
   "source": [
    "Next create a scatter plot using the 4th and 5th attributes. \n",
    "\n",
    "https://realpython.com/visualizing-python-plt-scatter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e941da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Attribute Definitions\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#     1.  fLength:  continuous  # major axis of ellipse [mm]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m#    10.  fDist:    continuous  # distance from origin to center of ellipse [mm]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m#    11.  class:    g,h         # gamma (signal), hadron (background)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m attribute_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mfLength\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfWidth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfSize\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfConc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfConc1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfAsym\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfM3Long\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfM3Trans\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfAlpha\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfDist\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "# Attribute Definitions\n",
    "#     1.  fLength:  continuous  # major axis of ellipse [mm]\n",
    "#     2.  fWidth:   continuous  # minor axis of ellipse [mm] \n",
    "#     3.  fSize:    continuous  # 10-log of sum of content of all pixels [in #phot]\n",
    "#     4.  fConc:    continuous  # ratio of sum of two highest pixels over fSize  [ratio]\n",
    "#     5.  fConc1:   continuous  # ratio of highest pixel over fSize  [ratio]\n",
    "#     6.  fAsym:    continuous  # distance from highest pixel to center, projected onto major axis [mm]\n",
    "#     7.  fM3Long:  continuous  # 3rd root of third moment along major axis  [mm] \n",
    "#     8.  fM3Trans: continuous  # 3rd root of third moment along minor axis  [mm]\n",
    "#     9.  fAlpha:   continuous  # angle of major axis with vector to origin [deg]\n",
    "#    10.  fDist:    continuous  # distance from origin to center of ellipse [mm]\n",
    "#    11.  class:    g,h         # gamma (signal), hadron (background)\n",
    "\n",
    "attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "MC_telescope_data = pd.read_csv(\"magic04.data\", header=None)  # tabular file header is absent\n",
    "MC_telescope_data.columns = attribute_names  # attach the column attribute labels\n",
    "MC_telescope_df = pd.DataFrame(MC_telescope_data)  # create a pandas dataframe \n",
    "\n",
    "\n",
    "MC_telescope_data_column_4 = MC_telescope_df.iloc[:,4]  #  \n",
    "MC_telescope_data_column_5 = MC_telescope_df.iloc[:,5]  # \n",
    "\n",
    "# KEEP THIS CODE FOR REFERENCE\n",
    "# plt.plot(MC_telescope_data_column_4, MC_telescope_data_column_5, \"o\")  # use plt.plot, if you want it to be fast.\n",
    "# plt.scatter(MC_telescope_data_column_4, MC_telescope_data_column_5)  # use plt.scatter, if you want more features.\n",
    "\n",
    "plt.scatter(x=MC_telescope_data_column_4, y=MC_telescope_data_column_5)  # use plt.scatter, if you want to use more features.\n",
    "plt.xlabel(attribute_names[3])  # \n",
    "plt.ylabel(attribute_names[4])  # \n",
    "plt.title(\"Scatter Plot of 4th and 5th Dimensions, MAGIC Telescope Data\")\n",
    "plt.savefig('scatter_plot_dimensions_4_5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1fa310",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70]\n",
    "interval_list = {'1-5':136, '6-15':181, '16-20':178, '21-50':695, '51-80':245, '81-110':177}  # Age:Frequency\n",
    "\n",
    "\n",
    "def lst_mean(lst):\n",
    "    result = np.mean(lst)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_median(lst):\n",
    "    result = np.median(lst)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_mode(lst):\n",
    "    result = stats.mode(lst, keepdims=True)\n",
    "    # return round(result, 3)\n",
    "    return result\n",
    "\n",
    "def lst_histogram(lst):\n",
    "    a = np.array(lst)\n",
    "    items = Counter(lst).keys()  # find the number of unique values in the set to set the bins\n",
    "    fig, ax = plt.subplots(figsize =(5, 3))\n",
    "    plt.hist(a, bins=range(np.min(lst)-5, np.max(lst)+5))\n",
    "\n",
    "def lst_multimode(lst):\n",
    "    res = []\n",
    "    test_list1 = Counter(lst)\n",
    "    temp = test_list1.most_common(1)[0][1]  # Extracts values of greatest frequency in the set.\n",
    "    for ele in lst:\n",
    "        if lst.count(ele) == temp:\n",
    "            res.append(ele)\n",
    "            res = list(set(res))\n",
    "    # printing results\n",
    "    return \"Data modality = \" + str(len(res)) + \"; mode list = \" + str(res)\n",
    "\n",
    "def lst_midrange(lst):\n",
    "    min = np.min(lst)\n",
    "    max = np.max(lst)\n",
    "    return round( ((min + max)/2.0), 3)\n",
    "\n",
    "def lst_q1(lst):\n",
    "    arr = np.array(lst)\n",
    "    result = np.quantile(arr, 0.25)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_q3(lst):\n",
    "    arr = np.array(lst)\n",
    "    result = np.quantile(arr, 0.75)\n",
    "    return round(result, 3)\n",
    "\n",
    "def cum_freq_list_from_inerval(dict):\n",
    "    cumulative_freq = 0\n",
    "    cumulative_freq_lst = []\n",
    "    for itm in dict:\n",
    "        cumulative_freq += dict[itm]  # collect the item frequency\n",
    "        cumulative_freq_lst.append(cumulative_freq)\n",
    "    n = cumulative_freq_lst[-1]\n",
    "    return cumulative_freq_lst\n",
    "    \n",
    "def find_n_by_2(dict):\n",
    "    cumulative_freq_lst = cum_freq_list_from_inerval(dict)\n",
    "    n = cumulative_freq_lst[-1]\n",
    "    n_by_2 = n/2\n",
    "    return n_by_2\n",
    "\n",
    "def find_median_bin(n_by_2, cumulative_freq_lst):\n",
    "    for i in range(len(cumulative_freq_lst)):\n",
    "        if n_by_2 <= cumulative_freq_lst[i]:\n",
    "            return i  # returns the index of the bin the median data point belongs to. \n",
    "        \n",
    "# def estimate_median(n_by_2, cumulative_freq_lst):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85845eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  29.963\n",
      "Q1:  20.5\n",
      "Q2:  25.0\n",
      "Q3:  35.0\n",
      "Data modality = 2; mode list = [25, 35]\n",
      "Midrange ((min+max)/2.0) :  41.5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMidrange ((min+max)/2.0) : \u001b[39m\u001b[39m\"\u001b[39m, lst_midrange(num_list))\n\u001b[0;32m     12\u001b[0m \u001b[39m# print(\"The data set is bimodal, with modes 25, 35.\")\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mprint\u001b[39m(lst_histogram(num_list))  \u001b[39m# use this to determine the mode(s) and modality of the set. \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mlst_histogram\u001b[1;34m(lst)\u001b[0m\n\u001b[0;32m     19\u001b[0m a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(lst)\n\u001b[0;32m     20\u001b[0m items \u001b[39m=\u001b[39m Counter(lst)\u001b[39m.\u001b[39mkeys()  \u001b[39m# find the number of unique values in the set to set the bins\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize \u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[0;32m     22\u001b[0m plt\u001b[39m.\u001b[39mhist(a, bins\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(np\u001b[39m.\u001b[39mmin(lst)\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m, np\u001b[39m.\u001b[39mmax(lst)\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# num_list = [13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70]\n",
    "# interval_list = {'1-5':136, '6-15':181, '16-20':178, '21-50':695, '51-80':245, '81-110':177}  # Age:Frequency\n",
    "\n",
    "# test the functions\n",
    "print(\"Mean: \", lst_mean(num_list))\n",
    "print(\"Q1: \", lst_q1(num_list))\n",
    "print(\"Q2: \", lst_median(num_list))\n",
    "print(\"Q3: \", lst_q3(num_list))\n",
    "print(lst_multimode(num_list))\n",
    "print(\"Midrange ((min+max)/2.0) : \", lst_midrange(num_list))\n",
    "\n",
    "# print(\"The data set is bimodal, with modes 25, 35.\")\n",
    "print(lst_histogram(num_list))  # use this to determine the mode(s) and modality of the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dc7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumm. frequency list: [136, 317, 495, 1190, 1435, 1612]\n",
      "n: 1612\n",
      "n/2: 806.0\n",
      "Group Index: 3\n",
      "Median data point resides in age group: '21-50'\n"
     ]
    }
   ],
   "source": [
    "# Question #4 on Computing Statistics using a Frequency Distribution Table\n",
    "# Estimated median = L + (L + ((n/2)-B)/G)*w\n",
    "# L is the lower class boundary of the group containing the median\n",
    "# n is the total number of values\n",
    "# B is the cumulative frequency of the groups before the median group\n",
    "# G is the frequency of the median group\n",
    "# w is the group width\n",
    "\n",
    "print(\"Cumm. frequency list:\", cum_freq_list_from_inerval(interval_list))\n",
    "print(\"n:\", cum_freq_list_from_inerval(interval_list)[-1])\n",
    "print(\"n/2:\", find_n_by_2(interval_list))\n",
    "print(\"Group Index:\", find_median_bin(find_n_by_2(interval_list), cum_freq_list_from_inerval(interval_list)))\n",
    "print(\"Median data point resides in age group: '21-50'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da1c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
