{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bed2bbc5-9d29-4c0b-812e-9fcbe775a582",
   "metadata": {},
   "source": [
    "<h1> Python Data Notebook (Template) </h1>\n",
    "<h3> Preserve this template, duplicate this file before using it!!</h3>\n",
    "<strong> Author: Micah Simmerman, Nolan Ollada, Nathan Palmer </strong>\n",
    "\n",
    "<strong>Resource URL:</strong>\n",
    "\n",
    "<strong>Database file(s), columnar dataset(s), etc.:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de39b907-6616-4c1f-8c75-8a020d020783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB file and Dependencies Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Libraries and packages\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "pd.__version__\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# SQLite db file connection is achieved in the try/except statements below. \n",
    "# TODO: add sections to read-in mySQL/PostGreSQL and csv files.\n",
    "\n",
    "db_file = '../California_Collision_Data.sqlite'  # name the db file name or resource file path.\n",
    "if os.path.isfile(db_file):  # determine if there is any SQLite db file by that name exists in the specified file location.\n",
    "    sqliteConnection = sqlite3.connect(db_file)  # establish a connection if the file does exist.\n",
    "    cursor = sqliteConnection.cursor()  # create a cursor object.\n",
    "    print('DB file and Dependencies Loaded.')  \n",
    "else:\n",
    "    print(\"No SQLite file detected.\")  # otherwise, print the connection status."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "327c63e2",
   "metadata": {},
   "source": [
    "Start by collecting information about the table names in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d094e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 tables found.\n",
      "Table and attribute name extraction completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# This cell performs a search operation to collects variables used to extract data from the db file \n",
    "#  \n",
    "# This cell performs the following actions:\n",
    "# 1.) Extracts the name of each table in the sqlite db file. \n",
    "# 2.) Stores the attributes of each table in a 2D list by generating a list of attributes (see the inner for-loop below), \n",
    "#       creating a so called \"list-of-lists\". Each sublist corresponds to the same index as its' corresponding table  \n",
    "#       element in the \"table\" list.\n",
    "# Note: Each of the lists and sublists generated in this cell may be accessed by other routines in this notebook. \n",
    "\n",
    "\n",
    "# Collect some schema informatio concerning the tables in the database file.\n",
    "table_query = 'SELECT name from sqlite_master where type= \"table\"'\n",
    "tables = []  # GLOBAL TABLE INDEX\n",
    "attribute_list = []  # GLOBAL ATTRIBUTE INDEX\n",
    "\n",
    "try:\n",
    "    # the first part of this algorithm collects db schema information and inserts the name of each table into a global list.\n",
    "    cursor.execute(table_query)\n",
    "    result = cursor.fetchall()\n",
    "    print()\n",
    "    print(len(result), \"tables found.\")\n",
    "    for i in range(len(result)):\n",
    "        tables.append(result[i][0])\n",
    "        # print(result[i][0])\n",
    "\n",
    "    # the second part of the algorithm extracts the attributes of each table identified in the first step and places them into a 2D list. \n",
    "    for table in tables:\n",
    "        consumer_complaints_count_records = \"PRAGMA table_info(\"\n",
    "        consumer_complaints_count_records += str(table)  + \");\"  # build the sqlite query string using the current list object\n",
    "        cursor.execute(consumer_complaints_count_records)  # execute the query string\n",
    "        result = cursor.fetchall()  # collect the results\n",
    "        temp_list = []\n",
    "        for item in result:\n",
    "            temp_list.append(item[1])\n",
    "        attribute_list.append(temp_list)  # export the list attribute table after it is built\n",
    "        # print(\"The\", table, \"table contains\", result[0][0], \"data points\")  # print the results\n",
    "        # print()\n",
    "    print(\"Table and attribute name extraction completed successfully.\")\n",
    "except:\n",
    "    print(\"Error:no db connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec84088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:  ['case_ids', 'victims', 'collisions', 'parties']\n",
      "Attribute list:  [['case_id', 'db_year'], ['id', 'case_id', 'party_number', 'victim_role', 'victim_sex', 'victim_age', 'victim_degree_of_injury', 'victim_seating_position', 'victim_safety_equipment_1', 'victim_safety_equipment_2', 'victim_ejected'], ['case_id', 'jurisdiction', 'reporting_district', 'population', 'county_city_location', 'county_location', 'chp_beat_type', 'primary_road', 'secondary_road', 'distance', 'direction', 'intersection', 'weather_1', 'weather_2', 'state_highway_indicator', 'caltrans_county', 'caltrans_district', 'state_route', 'route_suffix', 'postmile_prefix', 'postmile', 'location_type', 'ramp_intersection', 'side_of_highway', 'tow_away', 'collision_severity', 'killed_victims', 'injured_victims', 'party_count', 'primary_collision_factor', 'pcf_violation_code', 'pcf_violation_category', 'type_of_collision', 'motor_vehicle_involved_with', 'pedestrian_action', 'road_surface', 'road_condition_1', 'road_condition_2', 'lighting', 'control_device', 'chp_road_type', 'pedestrian_collision', 'bicycle_collision', 'motorcycle_collision', 'truck_collision', 'not_private_property', 'alcohol_involved', 'statewide_vehicle_type_at_fault', 'chp_vehicle_type_at_fault', 'severe_injury_count', 'other_visible_injury_count', 'complaint_of_pain_injury_count', 'pedestrian_killed_count', 'pedestrian_injured_count', 'bicyclist_killed_count', 'bicyclist_injured_count', 'motorcyclist_killed_count', 'motorcyclist_injured_count', 'primary_ramp', 'secondary_ramp', 'latitude', 'longitude', 'collision_date', 'collision_time'], ['id', 'case_id', 'party_number', 'party_type', 'at_fault', 'party_sex', 'party_age', 'party_sobriety', 'party_drug_physical', 'direction_of_travel', 'party_safety_equipment_1', 'party_safety_equipment_2', 'financial_responsibility', 'cellphone_in_use', 'cellphone_use_type', 'school_bus_related', 'oaf_violation_category', 'oaf_violation_section', 'oaf_violation_suffix', 'other_associate_factor_1', 'other_associate_factor_2', 'party_number_killed', 'party_number_injured', 'movement_preceding_collision', 'vehicle_year', 'vehicle_make', 'statewide_vehicle_type', 'chp_vehicle_type_towing', 'chp_vehicle_type_towed', 'party_race']]\n"
     ]
    }
   ],
   "source": [
    "# Now we have a list of tables and their respective attribute columns.\n",
    "print(\"Tables in the database: \", tables)\n",
    "print(\"Attribute list: \", attribute_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0dc1f9",
   "metadata": {},
   "source": [
    "### Database editing\n",
    "run the code below to remove the attributes in the remove_cols_collisions and remove_cols_parties tables specified. You can run the above 2 code blocks again to see that the specified attributes were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9debc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_column_string(old_columns, columns_to_remove):\n",
    "    new_columns = [col for col in old_columns if col not in columns_to_remove]\n",
    "    return \", \".join(new_columns)\n",
    "\n",
    "# The columns you want to remove\n",
    "remove_cols_collisions = ['officer_id', 'chp_shift', 'special_condition', \n",
    "                          'beat_type', 'city_division_lapd', 'chp_beat_class', \n",
    "                          'beat_number', 'pcf_violation', 'pcf_violation_subsection', \n",
    "                          'hit_and_run', 'process_date']\n",
    "remove_cols_parties = ['hazardous_materials', 'oaf_violation_code']\n",
    "\n",
    "# The new column strings\n",
    "new_cols_collisions = get_new_column_string(attribute_list[1], remove_cols_collisions)\n",
    "new_cols_parties = get_new_column_string(attribute_list[3], remove_cols_parties)\n",
    "\n",
    "# Create new tables without the specified columns\n",
    "cursor.execute(f'''\n",
    "    CREATE TABLE new_collisions AS \n",
    "    SELECT {new_cols_collisions} FROM collisions WHERE 1=0;\n",
    "''')\n",
    "\n",
    "cursor.execute(f'''\n",
    "    CREATE TABLE new_parties AS \n",
    "    SELECT {new_cols_parties} FROM parties WHERE 1=0;\n",
    "''')\n",
    "\n",
    "# Insert data from the old tables into the new ones\n",
    "cursor.execute(f'''\n",
    "    INSERT INTO new_collisions SELECT {new_cols_collisions} FROM collisions;\n",
    "''')\n",
    "\n",
    "cursor.execute(f'''\n",
    "    INSERT INTO new_parties SELECT {new_cols_parties} FROM parties;\n",
    "''')\n",
    "\n",
    "# Delete the old tables\n",
    "cursor.execute('DROP TABLE collisions;')\n",
    "cursor.execute('DROP TABLE parties;')\n",
    "\n",
    "# Rename the new tables to the original names\n",
    "cursor.execute('ALTER TABLE new_collisions RENAME TO collisions;')\n",
    "cursor.execute('ALTER TABLE new_parties RENAME TO parties;')\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "sqliteConnection.commit()\n",
    "sqliteConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bdb8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_entries_query = \"\"\"\n",
    "SELECT * FROM collisions\n",
    "JOIN parties ON collisions.case_id = parties.case_id\n",
    "JOIN victims ON parties.case_id = victims.case_id AND parties.party_number = victims.party_number\n",
    "ORDER BY RANDOM() \n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "random_entries = pd.read_sql_query(random_entries_query, sqliteConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7063affc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The case_ids table contains 9424334 data points\n",
      "The collisions table contains 9424334 data points\n",
      "The victims table contains 9639334 data points\n",
      "The parties table contains 18669166 data points\n"
     ]
    }
   ],
   "source": [
    "# This cell will determine the number of data points within each table of the database and enumerate them one by one.\n",
    "\n",
    "# attribute_list = []\n",
    "# count_records = 'SELECT COUNT(*) FROM ('  # start the string.\n",
    "# count_records += table + \");\"  # concatenate the variable and complete the string build.\n",
    "# cursor.execute(count_records)  # \n",
    "# result = cursor.fetchall()\n",
    "\n",
    "for table in range(len(tables)):\n",
    "    \n",
    "    count_records = 'SELECT COUNT(*) FROM ('  # start the string.\n",
    "    count_records += tables[table] + \");\"  # concatenate the variable and complete the string build.\n",
    "    cursor.execute(count_records)  # \n",
    "    result = cursor.fetchall()\n",
    "    print(\"The\", tables[table], \"table contains\", result[0][0], \"data points\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a5fc3c6",
   "metadata": {},
   "source": [
    "TO DO: INSERT BASIC STATISTICS FUNCTIONS BETWEEN THESE TWO CELLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180d15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will defines a first round Data Cube Function(s)\n",
    "create_data_cube = \"\"\"SELECT \n",
    "    c1, c2, AGGREGATE_FUNCTION(c3)\n",
    "FROM\n",
    "    table_name\n",
    "GROUP BY CUBE(c1 , c2);\"\"\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a666f304",
   "metadata": {},
   "source": [
    "The functions below currently only work on csv files. Add a conditional behavior to these functions and extend them to extract equivalent data from a table in an SQLite db file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160b7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are basic statistic functions that can be used as helper routines in longer extraction/cleaning/clustering efforts. \n",
    "# The remainder of the notebook contains code written to examine the telescope data. You can to generalize each function or create a\n",
    "# python scrip to examine tabular data files in a terminal window.\n",
    "\n",
    "# There are many methods that you can use to extract SQLite table data into a pandas dataframe or a numpy data object to generate graphics, \n",
    "# or perform package-based statistical testing. At this point in the notebook (and as of 11-June-2023).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# maximum value (max)\n",
    "def maximum(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)  # read the file into a pandas dataframe\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]  # project the indicated column from the pandas dataframe\n",
    "        result = column.max()  # find the maximum value\n",
    "        return round(result, 2)\n",
    "\n",
    "# minimum value (min)\n",
    "def minimum(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.min()  # find the minimum value\n",
    "        return round(result, 2)\n",
    "\n",
    "# average (mean)\n",
    "def mean(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.mean()  # calculate the average value\n",
    "        return round(result, 2)\n",
    "\n",
    "# standard deviation\n",
    "def std(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.std()  # calculate standard deviation\n",
    "        return round(result, 2)\n",
    "\n",
    "# first quantile, Q1\n",
    "def q1(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]\n",
    "        result = column.quantile([0.25]).values[0]\n",
    "        return round(result, 2)\n",
    "\n",
    "# third quantile, Q3\n",
    "def q3(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]\n",
    "        result = column.quantile([0.75]).values[0]\n",
    "        return round(result, 2)\n",
    "\n",
    "# median, Q2\n",
    "def median(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]  # slice the indiced column\n",
    "        result = column.quantile([0.5]).values[0]  # compute the second quantile\n",
    "        return round(result, 2)\n",
    "\n",
    "# inner-quartile range\n",
    "def iqr(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        iqr = float(q3(filename, attribute_number))-float(q1(filename, attribute_number))\n",
    "        # return \"{:.2f}\".format(iqr)  # format to 2 decimal places.\n",
    "        return round(iqr, 2)\n",
    "\n",
    "# N (number of objects)\n",
    "def count(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]  # slice the indicated column\n",
    "        result = column.count()  # count \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56661eb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# OUTPUT TESTING\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mN:\u001b[39m\u001b[39m\"\u001b[39m, count(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m7\u001b[39;49m))  \u001b[39m# N\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQ1:\u001b[39m\u001b[39m\"\u001b[39m, q1(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m7\u001b[39m))  \u001b[39m# Q1\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmedian:\u001b[39m\u001b[39m\"\u001b[39m, median(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m7\u001b[39m))  \u001b[39m# Median (Q2)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 98\u001b[0m, in \u001b[0;36mcount\u001b[1;34m(filename, attribute_number)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     MC_telescope_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(filename, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     99\u001b[0m     MC_telescope_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(MC_telescope_data)\n\u001b[0;32m    100\u001b[0m     column \u001b[39m=\u001b[39m MC_telescope_df\u001b[39m.\u001b[39miloc[:,attribute_number]  \u001b[39m# slice the indicated column\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# OUTPUT TESTING\n",
    "print(\"N:\", count(\"\", 7))  # N\n",
    "print(\"Q1:\", q1(\"\", 7))  # Q1\n",
    "print(\"median:\", median(\"\", 7))  # Median (Q2)\n",
    "print(\"Q3:\", q3(\"\", 7))  # Q3\n",
    "print(\"IQR:\", iqr(\"\", 7))  # IQR\n",
    "print(\"min:\", minimum(\"\", 7))  # min\n",
    "print(\"max:\", maximum(\"\", 7))  # max\n",
    "print(\"average:\", mean(\"\", 7))  # mean\n",
    "print(\"std. deviation:\", std(\"\", 7))  # standard deviation\n",
    "\n",
    "# # ERROR HANDLING\n",
    "# print(maximum(\"magic04.data\", 2.1))  # float (in-range)\n",
    "# print(minimum(\"magic04.data\", -1))  # out of bounds (less than)\n",
    "# print(mean(\"magic04.data\", 0))  # zero\n",
    "# print(std(\"magic04.data\", -1.5))  # negative out of range float\n",
    "# print(q1(\"magic04.data\", 10.5))  # positive out of range float\n",
    "# print(median(\"magic04.data\", 11))  # positive out of range (greater than)\n",
    "# print(q3(\"magic04.data\", -11.5))  # negative out of range float (less than)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b743b155",
   "metadata": {},
   "source": [
    "Next create a scatter plot using the 4th and 5th attributes. \n",
    "\n",
    "https://realpython.com/visualizing-python-plt-scatter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39e941da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Attribute Definitions\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#     1.  fLength:  continuous  # major axis of ellipse [mm]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m#    10.  fDist:    continuous  # distance from origin to center of ellipse [mm]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m#    11.  class:    g,h         # gamma (signal), hadron (background)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m attribute_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mfLength\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfWidth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfSize\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfConc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfConc1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfAsym\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfM3Long\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfM3Trans\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfAlpha\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfDist\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "# Attribute Definitions\n",
    "#     1.  fLength:  continuous  # major axis of ellipse [mm]\n",
    "#     2.  fWidth:   continuous  # minor axis of ellipse [mm] \n",
    "#     3.  fSize:    continuous  # 10-log of sum of content of all pixels [in #phot]\n",
    "#     4.  fConc:    continuous  # ratio of sum of two highest pixels over fSize  [ratio]\n",
    "#     5.  fConc1:   continuous  # ratio of highest pixel over fSize  [ratio]\n",
    "#     6.  fAsym:    continuous  # distance from highest pixel to center, projected onto major axis [mm]\n",
    "#     7.  fM3Long:  continuous  # 3rd root of third moment along major axis  [mm] \n",
    "#     8.  fM3Trans: continuous  # 3rd root of third moment along minor axis  [mm]\n",
    "#     9.  fAlpha:   continuous  # angle of major axis with vector to origin [deg]\n",
    "#    10.  fDist:    continuous  # distance from origin to center of ellipse [mm]\n",
    "#    11.  class:    g,h         # gamma (signal), hadron (background)\n",
    "\n",
    "attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "MC_telescope_data = pd.read_csv(\"magic04.data\", header=None)  # tabular file header is absent\n",
    "MC_telescope_data.columns = attribute_names  # attach the column attribute labels\n",
    "MC_telescope_df = pd.DataFrame(MC_telescope_data)  # create a pandas dataframe \n",
    "\n",
    "\n",
    "MC_telescope_data_column_4 = MC_telescope_df.iloc[:,4]  #  \n",
    "MC_telescope_data_column_5 = MC_telescope_df.iloc[:,5]  # \n",
    "\n",
    "# KEEP THIS CODE FOR REFERENCE\n",
    "# plt.plot(MC_telescope_data_column_4, MC_telescope_data_column_5, \"o\")  # use plt.plot, if you want it to be fast.\n",
    "# plt.scatter(MC_telescope_data_column_4, MC_telescope_data_column_5)  # use plt.scatter, if you want more features.\n",
    "\n",
    "plt.scatter(x=MC_telescope_data_column_4, y=MC_telescope_data_column_5)  # use plt.scatter, if you want to use more features.\n",
    "plt.xlabel(attribute_names[3])  # \n",
    "plt.ylabel(attribute_names[4])  # \n",
    "plt.title(\"Scatter Plot of 4th and 5th Dimensions, MAGIC Telescope Data\")\n",
    "plt.savefig('scatter_plot_dimensions_4_5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1fa310",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70]\n",
    "interval_list = {'1-5':136, '6-15':181, '16-20':178, '21-50':695, '51-80':245, '81-110':177}  # Age:Frequency\n",
    "\n",
    "\n",
    "def lst_mean(lst):\n",
    "    result = np.mean(lst)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_median(lst):\n",
    "    result = np.median(lst)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_mode(lst):\n",
    "    result = stats.mode(lst, keepdims=True)\n",
    "    # return round(result, 3)\n",
    "    return result\n",
    "\n",
    "def lst_histogram(lst):\n",
    "    a = np.array(lst)\n",
    "    items = Counter(lst).keys()  # find the number of unique values in the set to set the bins\n",
    "    fig, ax = plt.subplots(figsize =(5, 3))\n",
    "    plt.hist(a, bins=range(np.min(lst)-5, np.max(lst)+5))\n",
    "\n",
    "def lst_multimode(lst):\n",
    "    res = []\n",
    "    test_list1 = Counter(lst)\n",
    "    temp = test_list1.most_common(1)[0][1]  # Extracts values of greatest frequency in the set.\n",
    "    for ele in lst:\n",
    "        if lst.count(ele) == temp:\n",
    "            res.append(ele)\n",
    "            res = list(set(res))\n",
    "    # printing results\n",
    "    return \"Data modality = \" + str(len(res)) + \"; mode list = \" + str(res)\n",
    "\n",
    "def lst_midrange(lst):\n",
    "    min = np.min(lst)\n",
    "    max = np.max(lst)\n",
    "    return round( ((min + max)/2.0), 3)\n",
    "\n",
    "def lst_q1(lst):\n",
    "    arr = np.array(lst)\n",
    "    result = np.quantile(arr, 0.25)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_q3(lst):\n",
    "    arr = np.array(lst)\n",
    "    result = np.quantile(arr, 0.75)\n",
    "    return round(result, 3)\n",
    "\n",
    "def cum_freq_list_from_inerval(dict):\n",
    "    cumulative_freq = 0\n",
    "    cumulative_freq_lst = []\n",
    "    for itm in dict:\n",
    "        cumulative_freq += dict[itm]  # collect the item frequency\n",
    "        cumulative_freq_lst.append(cumulative_freq)\n",
    "    n = cumulative_freq_lst[-1]\n",
    "    return cumulative_freq_lst\n",
    "    \n",
    "def find_n_by_2(dict):\n",
    "    cumulative_freq_lst = cum_freq_list_from_inerval(dict)\n",
    "    n = cumulative_freq_lst[-1]\n",
    "    n_by_2 = n/2\n",
    "    return n_by_2\n",
    "\n",
    "def find_median_bin(n_by_2, cumulative_freq_lst):\n",
    "    for i in range(len(cumulative_freq_lst)):\n",
    "        if n_by_2 <= cumulative_freq_lst[i]:\n",
    "            return i  # returns the index of the bin the median data point belongs to. \n",
    "        \n",
    "# def estimate_median(n_by_2, cumulative_freq_lst):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85845eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  29.963\n",
      "Q1:  20.5\n",
      "Q2:  25.0\n",
      "Q3:  35.0\n",
      "Data modality = 2; mode list = [25, 35]\n",
      "Midrange ((min+max)/2.0) :  41.5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMidrange ((min+max)/2.0) : \u001b[39m\u001b[39m\"\u001b[39m, lst_midrange(num_list))\n\u001b[0;32m     12\u001b[0m \u001b[39m# print(\"The data set is bimodal, with modes 25, 35.\")\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mprint\u001b[39m(lst_histogram(num_list))  \u001b[39m# use this to determine the mode(s) and modality of the set. \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mlst_histogram\u001b[1;34m(lst)\u001b[0m\n\u001b[0;32m     19\u001b[0m a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(lst)\n\u001b[0;32m     20\u001b[0m items \u001b[39m=\u001b[39m Counter(lst)\u001b[39m.\u001b[39mkeys()  \u001b[39m# find the number of unique values in the set to set the bins\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize \u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[0;32m     22\u001b[0m plt\u001b[39m.\u001b[39mhist(a, bins\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(np\u001b[39m.\u001b[39mmin(lst)\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m, np\u001b[39m.\u001b[39mmax(lst)\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# num_list = [13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70]\n",
    "# interval_list = {'1-5':136, '6-15':181, '16-20':178, '21-50':695, '51-80':245, '81-110':177}  # Age:Frequency\n",
    "\n",
    "# test the functions\n",
    "print(\"Mean: \", lst_mean(num_list))\n",
    "print(\"Q1: \", lst_q1(num_list))\n",
    "print(\"Q2: \", lst_median(num_list))\n",
    "print(\"Q3: \", lst_q3(num_list))\n",
    "print(lst_multimode(num_list))\n",
    "print(\"Midrange ((min+max)/2.0) : \", lst_midrange(num_list))\n",
    "\n",
    "# print(\"The data set is bimodal, with modes 25, 35.\")\n",
    "print(lst_histogram(num_list))  # use this to determine the mode(s) and modality of the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e68dc7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumm. frequency list: [136, 317, 495, 1190, 1435, 1612]\n",
      "n: 1612\n",
      "n/2: 806.0\n",
      "Group Index: 3\n",
      "Median data point resides in age group: '21-50'\n"
     ]
    }
   ],
   "source": [
    "# Question #4 on Computing Statistics using a Frequency Distribution Table\n",
    "# Estimated median = L + (L + ((n/2)-B)/G)*w\n",
    "# L is the lower class boundary of the group containing the median\n",
    "# n is the total number of values\n",
    "# B is the cumulative frequency of the groups before the median group\n",
    "# G is the frequency of the median group\n",
    "# w is the group width\n",
    "\n",
    "print(\"Cumm. frequency list:\", cum_freq_list_from_inerval(interval_list))\n",
    "print(\"n:\", cum_freq_list_from_inerval(interval_list)[-1])\n",
    "print(\"n/2:\", find_n_by_2(interval_list))\n",
    "print(\"Group Index:\", find_median_bin(find_n_by_2(interval_list), cum_freq_list_from_inerval(interval_list)))\n",
    "print(\"Median data point resides in age group: '21-50'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da1c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
