{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bed2bbc5-9d29-4c0b-812e-9fcbe775a582",
   "metadata": {},
   "source": [
    "<h1> Python Data Notebook (Template) </h1>\n",
    "<h3> Preserve this template, duplicate this file before using it!!</h3>\n",
    "<strong> Author: Micah Simmerman, Nolan Ollada, Nathan Palmer </strong>\n",
    "\n",
    "<strong>Resource URL:</strong>\n",
    "\n",
    "<strong>Database file(s), columnar dataset(s), etc.:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de39b907-6616-4c1f-8c75-8a020d020783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 tables found.\n",
      "Table and attribute name extraction completed successfully.\n",
      "DB file and Dependencies Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Libraries and packages\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "pd.__version__\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# SQLite db file connection is achieved in the try/except statements below. \n",
    "# TODO: add sections to read-in mySQL/PostGreSQL and csv files.\n",
    "\n",
    "tables = []  # GLOBAL TABLE INDEX\n",
    "attribute_list = []  # GLOBAL ATTRIBUTE INDEX\n",
    "\n",
    "# NOTE: always store the switrs.sqlite file in the parent directory where this python notebook is kept.\n",
    "db_file = '../switrs.sqlite'  # name the db file downloaded directly from https://www.kaggle.com/datasets/alexgude/california-traffic-collision-data-from-switrs\n",
    "sqliteConnection = sqlite3.connect(db_file)\n",
    "\n",
    "# if os.path.isfile(db_file):  # determine if there is any SQLite db file by that name exists in the specified file location.\n",
    "sqliteConnection = sqlite3.connect(db_file)  # establish a connection if the file does exist.\n",
    "cursor = sqliteConnection.cursor()  # create a cursor object.\n",
    "table_query = 'SELECT name from sqlite_master where type= \"table\"'\n",
    "cursor.execute(table_query)\n",
    "result = cursor.fetchall()\n",
    "print()\n",
    "print(len(result), \"tables found.\")\n",
    "for i in range(len(result)):\n",
    "    tables.append(result[i][0])\n",
    "    # print(result[i][0])\n",
    "\n",
    "# the second part of the algorithm extracts the attributes of each table identified in the first step and places them into a 2D list. \n",
    "for table in tables:\n",
    "    consumer_complaints_count_records = \"PRAGMA table_info(\"\n",
    "    consumer_complaints_count_records += str(table)  + \");\"  # build the sqlite query string using the current list object\n",
    "    cursor.execute(consumer_complaints_count_records)  # execute the query string\n",
    "    result = cursor.fetchall()  # collect the results\n",
    "    temp_list = []\n",
    "    for item in result:\n",
    "        temp_list.append(item[1])\n",
    "    attribute_list.append(temp_list)  # export the list attribute table after it is built\n",
    "    # print(\"The\", table, \"table contains\", result[0][0], \"data points\")  # print the results\n",
    "    # print()\n",
    "print(\"Table and attribute name extraction completed successfully.\")\n",
    "\n",
    "\n",
    "print('DB file and Dependencies Loaded.')  \n",
    "# else:\n",
    "#     print(os.path.isfile(db_file), \"No SQLite file detected.\")  # otherwise, print the connection status."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "327c63e2",
   "metadata": {},
   "source": [
    "Start by collecting information about the table names in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec84088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:  ['case_ids', 'collisions', 'victims', 'parties']\n",
      "Attribute list:  [['case_id', 'db_year'], ['case_id', 'jurisdiction', 'reporting_district', 'population', 'county_city_location', 'county_location', 'chp_beat_type', 'primary_road', 'secondary_road', 'distance', 'direction', 'intersection', 'weather_1', 'weather_2', 'state_highway_indicator', 'caltrans_county', 'caltrans_district', 'state_route', 'route_suffix', 'postmile_prefix', 'postmile', 'location_type', 'ramp_intersection', 'side_of_highway', 'tow_away', 'collision_severity', 'killed_victims', 'injured_victims', 'party_count', 'primary_collision_factor', 'pcf_violation_code', 'pcf_violation_category', 'type_of_collision', 'motor_vehicle_involved_with', 'pedestrian_action', 'road_surface', 'road_condition_1', 'road_condition_2', 'lighting', 'control_device', 'chp_road_type', 'pedestrian_collision', 'bicycle_collision', 'motorcycle_collision', 'truck_collision', 'not_private_property', 'alcohol_involved', 'statewide_vehicle_type_at_fault', 'chp_vehicle_type_at_fault', 'severe_injury_count', 'other_visible_injury_count', 'complaint_of_pain_injury_count', 'pedestrian_killed_count', 'pedestrian_injured_count', 'bicyclist_killed_count', 'bicyclist_injured_count', 'motorcyclist_killed_count', 'motorcyclist_injured_count', 'primary_ramp', 'secondary_ramp', 'latitude', 'longitude', 'collision_date', 'collision_time'], ['id', 'case_id', 'party_number', 'victim_role', 'victim_sex', 'victim_age', 'victim_degree_of_injury', 'victim_seating_position', 'victim_safety_equipment_1', 'victim_safety_equipment_2', 'victim_ejected'], ['id', 'case_id', 'party_number', 'party_type', 'at_fault', 'party_sex', 'party_age', 'party_sobriety', 'party_drug_physical', 'direction_of_travel', 'party_safety_equipment_1', 'party_safety_equipment_2', 'financial_responsibility', 'cellphone_in_use', 'cellphone_use_type', 'school_bus_related', 'oaf_violation_category', 'oaf_violation_section', 'oaf_violation_suffix', 'other_associate_factor_1', 'other_associate_factor_2', 'party_number_killed', 'party_number_injured', 'movement_preceding_collision', 'vehicle_year', 'vehicle_make', 'statewide_vehicle_type', 'chp_vehicle_type_towing', 'chp_vehicle_type_towed', 'party_race']]\n"
     ]
    }
   ],
   "source": [
    "# Now we have a list of tables and their respective attribute columns.\n",
    "print(\"Tables in the database: \", tables)\n",
    "print(\"Attribute list: \", attribute_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0dc1f9",
   "metadata": {},
   "source": [
    "### Database editing\n",
    "run the code below to remove the attributes in the remove_cols_collisions and remove_cols_parties tables specified. You can run the above 2 code blocks again to see that the specified attributes were removed.\n",
    "\n",
    "Running the following cell twice currently results in an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d6c78",
   "metadata": {},
   "source": [
    "    RATIONAL FOR ATTRIBUTE COLUMN REMOVAL:\n",
    "    collisions.officer_id: responding officers identification is not relevant to the factors of a collision.\n",
    "    collisions.chp_shift: chp_shift indicates the work shift period of the responding officer. More accurate temporal measurements exist.\n",
    "    collisions.special_condition: \n",
    "    collisions.beat_type: \n",
    "    collisions.city_division_lapd: \n",
    "    collisions.chp_beat_class: \n",
    "    collisions.beat_number: \n",
    "    collisions.pcf_violation: \n",
    "    collisions.pcf_violation_subsection: \n",
    "    collisions.hit_and_run: \n",
    "    collisions.process_date: \n",
    "\n",
    "    For a complete list of attribute definitions, please visit: https://peteraldhous.com/Data/ca_traffic/SWITRS_codebook.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77330f",
   "metadata": {},
   "source": [
    "# Warning:\n",
    "The following cell block removes unwanted attribute columns from the SWITRS database file and will take a long time to execute. Repeated requests should not impose harm on the database file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965fcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the page count to \n",
    "increase_page_count = \"PRAGMA max_page_count = 2147483646;\"\n",
    "turn_off_journal = \"PRAGMA journal_mode=OFF\"\n",
    "cursor.execute(increase_page_count)\n",
    "cursor.execute(turn_off_journal)  # turn off the journaling mode (temporarily) to avoid exceeding local disk space limitations.\n",
    "sqliteConnection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e10693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed officer_id from the 'collisions' table.\n",
      "working...\n",
      "Removed chp_shift from the 'collisions' table.\n",
      "working...\n",
      "Removed special_condition from the 'collisions' table.\n",
      "working...\n",
      "Removed beat_type from the 'collisions' table.\n",
      "working...\n",
      "Removed city_division_lapd from the 'collisions' table.\n",
      "working...\n",
      "Removed chp_beat_class from the 'collisions' table.\n",
      "working...\n",
      "Removed beat_number from the 'collisions' table.\n",
      "working...\n",
      "Removed pcf_violation from the 'collisions' table.\n",
      "working...\n",
      "Removed pcf_violation_subsection from the 'collisions' table.\n",
      "working...\n",
      "Removed hit_and_run from the 'collisions' table.\n",
      "working...\n",
      "Removed process_date from the 'collisions' table.\n",
      "working...\n",
      "Removed hazardous_materials from the 'parties' table\n",
      "working...\n",
      "Removed oaf_violation_code from the 'parties' table\n",
      "working...\n",
      "Database tables dropped successfully.\n",
      "journal mode is back on.\n"
     ]
    }
   ],
   "source": [
    "# The columns you want to remove\n",
    "remove_cols_collisions = [\"officer_id\", \"chp_shift\", \"special_condition\", \"beat_type\", \"city_division_lapd\", \"chp_beat_class\", \"beat_number\", \"pcf_violation\", \"pcf_violation_subsection\", \"hit_and_run\", \"process_date\"]\n",
    "# remove_cols_collisions = \"officer_id, chp_shift, special_condition, beat_type, city_division_lapd, chp_beat_class, beat_number, pcf_violation, pcf_violation_subsection, hit_and_run, process_date\"\n",
    "remove_cols_parties = [\"hazardous_materials\", \"oaf_violation_code\"]\n",
    "# remove_cols_parties = \"hazardous_materials, oaf_violation_code\"\n",
    "\n",
    "for col in remove_cols_collisions:\n",
    "    drop_collisions_columns = \"ALTER TABLE collisions DROP  \"\n",
    "    drop_collisions_columns += col  + \";\" \n",
    "    cursor.execute(drop_collisions_columns)\n",
    "    sqliteConnection.commit()\n",
    "    print(\"Removed\", col, \"from the 'collisions' table.\")\n",
    "    print(\"working...\")\n",
    "for attrib in remove_cols_parties:\n",
    "    drop_parties_columns = \"ALTER TABLE parties DROP \"\n",
    "    drop_parties_columns += attrib  + \";\" \n",
    "    cursor.execute(drop_parties_columns)\n",
    "    sqliteConnection.commit()\n",
    "    print(\"Removed\", attrib, \"from the 'parties' table\")\n",
    "    print(\"working...\")\n",
    "print(\"Database tables dropped successfully.\")\n",
    "turn_journal_on = \"PRAGMA journal_mode=ON\"\n",
    "cursor.execute(turn_journal_on)  # turn the journaling mode back on so we can roll-back our changes.\n",
    "sqliteConnection.commit()  # commit the change to the database file.\n",
    "print(\"journal mode is back on.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f19249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_ids has attributes: ['case_id', 'db_year']\n",
      "collisions has attributes: ['case_id', 'jurisdiction', 'reporting_district', 'population', 'county_city_location', 'county_location', 'chp_beat_type', 'primary_road', 'secondary_road', 'distance', 'direction', 'intersection', 'weather_1', 'weather_2', 'state_highway_indicator', 'caltrans_county', 'caltrans_district', 'state_route', 'route_suffix', 'postmile_prefix', 'postmile', 'location_type', 'ramp_intersection', 'side_of_highway', 'tow_away', 'collision_severity', 'killed_victims', 'injured_victims', 'party_count', 'primary_collision_factor', 'pcf_violation_code', 'pcf_violation_category', 'type_of_collision', 'motor_vehicle_involved_with', 'pedestrian_action', 'road_surface', 'road_condition_1', 'road_condition_2', 'lighting', 'control_device', 'chp_road_type', 'pedestrian_collision', 'bicycle_collision', 'motorcycle_collision', 'truck_collision', 'not_private_property', 'alcohol_involved', 'statewide_vehicle_type_at_fault', 'chp_vehicle_type_at_fault', 'severe_injury_count', 'other_visible_injury_count', 'complaint_of_pain_injury_count', 'pedestrian_killed_count', 'pedestrian_injured_count', 'bicyclist_killed_count', 'bicyclist_injured_count', 'motorcyclist_killed_count', 'motorcyclist_injured_count', 'primary_ramp', 'secondary_ramp', 'latitude', 'longitude', 'collision_date', 'collision_time']\n",
      "victims has attributes: ['id', 'case_id', 'party_number', 'victim_role', 'victim_sex', 'victim_age', 'victim_degree_of_injury', 'victim_seating_position', 'victim_safety_equipment_1', 'victim_safety_equipment_2', 'victim_ejected']\n",
      "parties has attributes: ['id', 'case_id', 'party_number', 'party_type', 'at_fault', 'party_sex', 'party_age', 'party_sobriety', 'party_drug_physical', 'direction_of_travel', 'party_safety_equipment_1', 'party_safety_equipment_2', 'financial_responsibility', 'cellphone_in_use', 'cellphone_use_type', 'school_bus_related', 'oaf_violation_category', 'oaf_violation_section', 'oaf_violation_suffix', 'other_associate_factor_1', 'other_associate_factor_2', 'party_number_killed', 'party_number_injured', 'movement_preceding_collision', 'vehicle_year', 'vehicle_make', 'statewide_vehicle_type', 'chp_vehicle_type_towing', 'chp_vehicle_type_towed', 'party_race']\n"
     ]
    }
   ],
   "source": [
    "# Now we have a list of tables and their respective attribute columns.+\n",
    "for i in range(len(tables)):\n",
    "    print(tables[i], \"has attributes:\", attribute_list[i])\n",
    "    # print(attribute_list[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9debc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_new_column_string(old_columns, columns_to_remove):\n",
    "#     new_columns = [col for col in old_columns if col not in columns_to_remove]\n",
    "#     return \", \".join(new_columns)\n",
    "\n",
    "# # The columns you want to remove\n",
    "# remove_cols_collisions = ['officer_id', 'chp_shift', 'special_condition', \n",
    "#                           'beat_type', 'city_division_lapd', 'chp_beat_class', \n",
    "#                           'beat_number', 'pcf_violation', 'pcf_violation_subsection', \n",
    "#                           'hit_and_run', 'process_date']\n",
    "# remove_cols_parties = ['hazardous_materials', 'oaf_violation_code']\n",
    "\n",
    "\n",
    "# # The new column strings\n",
    "# new_cols_collisions = get_new_column_string(attribute_list[1], remove_cols_collisions)\n",
    "# new_cols_parties = get_new_column_string(attribute_list[3], remove_cols_parties)\n",
    "\n",
    "# # Create new tables without the specified columns\n",
    "# cursor.execute(f'''\n",
    "#     CREATE TABLE new_collisions AS \n",
    "#     SELECT {new_cols_collisions} FROM collisions WHERE 1=0;\n",
    "# ''')\n",
    "\n",
    "# cursor.execute(f'''\n",
    "#     CREATE TABLE new_parties AS \n",
    "#     SELECT {new_cols_parties} FROM parties WHERE 1=0;\n",
    "# ''')\n",
    "\n",
    "# # Insert data from the old tables into the new ones\n",
    "# cursor.execute(f'''\n",
    "#     INSERT INTO new_collisions SELECT {new_cols_collisions} FROM collisions;\n",
    "# ''')\n",
    "\n",
    "# cursor.execute(f'''\n",
    "#     INSERT INTO new_parties SELECT {new_cols_parties} FROM parties;\n",
    "# ''')\n",
    "\n",
    "# # Delete the old tables\n",
    "# cursor.execute('DROP TABLE collisions;')\n",
    "# cursor.execute('DROP TABLE parties;')\n",
    "\n",
    "# # Rename the new tables to the original names\n",
    "# cursor.execute('ALTER TABLE new_collisions RENAME TO collisions;')\n",
    "# cursor.execute('ALTER TABLE new_parties RENAME TO parties;')\n",
    "\n",
    "# # Commit the changes and close the connection\n",
    "# sqliteConnection.commit()\n",
    "# # sqliteConnection.close()  # this will create a \"cannot operate on a closed database\" error in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bdb8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Collect a random sample of data points using a join query composed of a 3-way join on attribute 'case_id'.\n",
    "# random_entries_query = \"\"\"\n",
    "# SELECT * FROM collisions\n",
    "# JOIN parties ON collisions.case_id = parties.case_id\n",
    "# JOIN victims ON parties.case_id = victims.case_id AND parties.party_number = victims.party_number\n",
    "# ORDER BY RANDOM() \n",
    "# LIMIT 10\n",
    "# \"\"\"\n",
    "# random_entries = pd.read_sql_query(random_entries_query, sqliteConnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7063affc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The case_ids table contains 9424334 data points\n",
      "The collisions table contains 9424334 data points\n",
      "The victims table contains 9639334 data points\n",
      "The parties table contains 18669166 data points\n"
     ]
    }
   ],
   "source": [
    "# This cell will determine the number of data points within each table of the database and enumerate them one by one.\n",
    "\n",
    "# attribute_list = []\n",
    "# count_records = 'SELECT COUNT(*) FROM ('  # start the string.\n",
    "# count_records += table + \");\"  # concatenate the variable and complete the string build.\n",
    "# cursor.execute(count_records)  # \n",
    "# result = cursor.fetchall()\n",
    "\n",
    "for table in range(len(tables)):\n",
    "    count_records = 'SELECT COUNT(*) FROM ('  # start the string.\n",
    "    count_records += tables[table] + \");\"  # concatenate the variable and complete the string build.\n",
    "    cursor.execute(count_records)  # \n",
    "    result = cursor.fetchall()\n",
    "    print(\"The\", tables[table], \"table contains\", result[0][0], \"data points\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a5fc3c6",
   "metadata": {},
   "source": [
    "TO DO: FILL IN THE FOLLOWING CODE STUBS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c0454",
   "metadata": {},
   "source": [
    "### Data Normalization and Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "947794c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               case_id  jurisdiction reporting_district        population  \\\n",
      "0              2698369          5604               None  100000 to 250000   \n",
      "1              3965885           107               None   50000 to 100000   \n",
      "2             81466685          1005              SOUTH           >250000   \n",
      "3             91049156          9255               None    unincorporated   \n",
      "4  1900010214082099999          1900                332    25000 to 50000   \n",
      "\n",
      "  county_city_location county_location chp_beat_type        primary_road  \\\n",
      "0                 5604         ventura       not chp  CHANNEL ISLANDS BL   \n",
      "1                 0107         alameda       not chp        LINDBERGH AV   \n",
      "2                 1005          fresno       not chp      S MERIDIAN AVE   \n",
      "3                 0500       calaveras   state route      STATE ROUTE 26   \n",
      "4                 1939     los angeles       not chp        INGLEWOOD AV   \n",
      "\n",
      "     secondary_road  distance  ... bicyclist_killed_count  \\\n",
      "0         CLOYNE ST      75.0  ...                      0   \n",
      "1      ARMSTRONG ST     177.0  ...                      0   \n",
      "2  E HUNTINGTON AVE     200.0  ...                      0   \n",
      "3           PINE ST     180.0  ...                      0   \n",
      "4               147     171.0  ...                      0   \n",
      "\n",
      "   bicyclist_injured_count motorcyclist_killed_count  \\\n",
      "0                        0                         0   \n",
      "1                        0                         0   \n",
      "2                        0                         0   \n",
      "3                        0                         0   \n",
      "4                        0                         0   \n",
      "\n",
      "  motorcyclist_injured_count  primary_ramp secondary_ramp  latitude  \\\n",
      "0                          0          None           None       NaN   \n",
      "1                          0          None           None       NaN   \n",
      "2                          0          None           None       NaN   \n",
      "3                          0          None           None  38.39995   \n",
      "4                          0          None           None       NaN   \n",
      "\n",
      "   longitude collision_date collision_time  \n",
      "0        NaN     2006-06-24       02:51:00  \n",
      "1        NaN     2008-10-28       06:58:00  \n",
      "2        NaN     2021-05-01       17:00:00  \n",
      "3 -120.52652     2019-08-03       08:45:00  \n",
      "4        NaN     2001-02-14       08:20:00  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SAMPLER STUB FOR 'collisions' table\n",
    "# Read 100 randomly selected sqlite table query tuples into a local pandas DataFrame\n",
    "collisions_random_100_df = pd.read_sql_query(\"SELECT * FROM collisions ORDER BY RANDOM() LIMIT 100;\", sqliteConnection)\n",
    "print(collisions_random_100_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de88f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id   case_id  party_number party_type  at_fault party_sex  party_age  \\\n",
      "0  8638779  90619322             1     driver         1    female       25.0   \n",
      "1  5260843   2831696             1     driver         1      male       69.0   \n",
      "2  2181321   1210527             1     driver         1    female       19.0   \n",
      "3   460582   0244374             1     driver         1    female       79.0   \n",
      "4  3978107   6368489             2     driver         0      male       56.0   \n",
      "\n",
      "                       party_sobriety party_drug_physical direction_of_travel  \\\n",
      "0  had been drinking, under influence                None                east   \n",
      "1               had not been drinking                None                east   \n",
      "2               had not been drinking                None                east   \n",
      "3               had not been drinking                None                east   \n",
      "4               had not been drinking                None                west   \n",
      "\n",
      "   ... other_associate_factor_2 party_number_killed party_number_injured  \\\n",
      "0  ...                     None                   0                    0   \n",
      "1  ...                     None                   0                    1   \n",
      "2  ...                     None                   0                    0   \n",
      "3  ...                     None                   0                    0   \n",
      "4  ...                     None                   0                    1   \n",
      "\n",
      "   movement_preceding_collision vehicle_year  vehicle_make  \\\n",
      "0                changing lanes       2014.0     chevrolet   \n",
      "1              making left turn       1999.0         dodge   \n",
      "2           proceeding straight       1985.0          ford   \n",
      "3           proceeding straight       1968.0          ford   \n",
      "4                       stopped       2012.0        nissan   \n",
      "\n",
      "  statewide_vehicle_type chp_vehicle_type_towing chp_vehicle_type_towed  \\\n",
      "0  pickup or panel truck        pickups & panels                   None   \n",
      "1                   None                    None                     00   \n",
      "2          passenger car                    None                   None   \n",
      "3          passenger car                    None                   None   \n",
      "4          passenger car   sport utility vehicle                     00   \n",
      "\n",
      "  party_race  \n",
      "0      asian  \n",
      "1      other  \n",
      "2       None  \n",
      "3      black  \n",
      "4      white  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SAMPLER STUB FOR 'parties' table\n",
    "# Read 100 randomly selected sqlite table query tuples into a local pandas DataFrame\n",
    "parties_random_100_df = pd.read_sql_query(\"SELECT * FROM parties ORDER BY RANDOM() LIMIT 100;\", sqliteConnection)\n",
    "print(parties_random_100_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7f9405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    case_id  jurisdiction reporting_district      population  \\\n",
      "0  90332665          9375               None  unincorporated   \n",
      "1   4076500          3801               None         >250000   \n",
      "2   9074191          1962               1962  25000 to 50000   \n",
      "3   3397405          1900               0972  25000 to 50000   \n",
      "4   6321202          4202              LOMPO  25000 to 50000   \n",
      "\n",
      "  county_city_location county_location     chp_beat_type   primary_road  \\\n",
      "0                 0100         alameda  county road area  GREENVILLE PL   \n",
      "1                 3801   san francisco           not chp    VAN NESS AV   \n",
      "2                 1962     los angeles           not chp    MUSCATEL AV   \n",
      "3                 1979     los angeles           not chp     MELROSE AV   \n",
      "4                 4202   santa barbara           not chp           A ST   \n",
      "\n",
      "  secondary_road  distance  ... bicyclist_killed_count  \\\n",
      "0  PINEVILLE CIR     180.0  ...                      0   \n",
      "1        EDDY ST       0.0  ...                      0   \n",
      "2         ELM AV       0.0  ...                      0   \n",
      "3  LA CIENEGA BL       0.0  ...                      0   \n",
      "4        PINE AV       0.0  ...                      0   \n",
      "\n",
      "   bicyclist_injured_count motorcyclist_killed_count  \\\n",
      "0                        0                         0   \n",
      "1                        0                         0   \n",
      "2                        0                         0   \n",
      "3                        0                         0   \n",
      "4                        0                         0   \n",
      "\n",
      "  motorcyclist_injured_count  primary_ramp secondary_ramp  latitude  \\\n",
      "0                          0          None           None  37.71655   \n",
      "1                          0          None           None       NaN   \n",
      "2                          0          None           None  34.10490   \n",
      "3                          0          None           None       NaN   \n",
      "4                          0          None           None       NaN   \n",
      "\n",
      "   longitude collision_date collision_time  \n",
      "0 -122.02155     2016-11-24       20:20:00  \n",
      "1        NaN     2008-12-17       21:34:00  \n",
      "2 -118.07723     2020-03-17       07:25:00  \n",
      "3        NaN     2007-09-22       21:52:00  \n",
      "4        NaN     2013-11-25       14:26:00  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SAMPLER STUB FOR 'victims' table\n",
    "# Read 100 randomly selected sqlite table query tuples into a local pandas DataFrame\n",
    "victims_random_100_df = pd.read_sql_query(\"SELECT * FROM victims ORDER BY RANDOM() LIMIT 100;\", sqliteConnection)\n",
    "print(victims_random_100_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e7ecfe1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "database or disk is full",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create a view using SQLite so that we can access all data as if it were a single data point object.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Since collisions, parties, and victims contain all of the information we need, we would like to join these three tables to create our random samples.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m create_sql_view \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39m \u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39m                    SELECT * \u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39m                    FROM collisions\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m                    LIMIT 100;\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39m                \u001b[39m\u001b[39m\"\"\"\u001b[39m \n\u001b[1;32m---> 11\u001b[0m cursor\u001b[39m.\u001b[39;49mexecute(create_sql_view)\n\u001b[0;32m     12\u001b[0m result \u001b[39m=\u001b[39m cursor\u001b[39m.\u001b[39mfetchall()\n",
      "\u001b[1;31mOperationalError\u001b[0m: database or disk is full"
     ]
    }
   ],
   "source": [
    "# # Create a view using SQLite so that we can access all data as if it were a single data point object.\n",
    "# # Since collisions, parties, and victims contain all of the information we need, we would like to join these three tables to create our random samples.\n",
    "# create_sql_view = \"\"\" \n",
    "#                     SELECT * \n",
    "#                     FROM collisions\n",
    "#                     INNER JOIN parties ON collisions.case_id = parties.case_id\n",
    "#                     INNER JOIN victims ON parties.case_id = victims.case_id AND parties.party_number = victims.party_number\n",
    "#                     ORDER BY RANDOM() \n",
    "#                     LIMIT 100;\n",
    "#                 \"\"\" \n",
    "# cursor.execute(create_sql_view)\n",
    "# result = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will defines a first round Data Cube Function(s)\n",
    "create_data_cube = \"\"\"SELECT \n",
    "    c1, c2, AGGREGATE_FUNCTION(c3)\n",
    "FROM\n",
    "    table_name\n",
    "GROUP BY CUBE(c1 , c2);\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a666f304",
   "metadata": {},
   "source": [
    "The functions below currently only work on csv files. Add a conditional behavior to these functions and extend them to extract equivalent data from a table in an SQLite db file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are basic statistic functions that can be used as helper routines in longer extraction/cleaning/clustering efforts. \n",
    "# The remainder of the notebook contains code written to examine the telescope data. You can to generalize each function or create a\n",
    "# python scrip to examine tabular data files in a terminal window.\n",
    "\n",
    "# There are many methods that you can use to extract SQLite table data into a pandas dataframe or a numpy data object to generate graphics, \n",
    "# or perform package-based statistical testing. At this point in the notebook (and as of 11-June-2023).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# maximum value (max)\n",
    "def maximum(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)  # read the file into a pandas dataframe\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]  # project the indicated column from the pandas dataframe\n",
    "        result = column.max()  # find the maximum value\n",
    "        return round(result, 2)\n",
    "\n",
    "# minimum value (min)\n",
    "def minimum(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.min()  # find the minimum value\n",
    "        return round(result, 2)\n",
    "\n",
    "# average (mean)\n",
    "def mean(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.mean()  # calculate the average value\n",
    "        return round(result, 2)\n",
    "\n",
    "# standard deviation\n",
    "def std(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.std()  # calculate standard deviation\n",
    "        return round(result, 2)\n",
    "\n",
    "# first quantile, Q1\n",
    "def q1(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]\n",
    "        result = column.quantile([0.25]).values[0]\n",
    "        return round(result, 2)\n",
    "\n",
    "# third quantile, Q3\n",
    "def q3(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]\n",
    "        result = column.quantile([0.75]).values[0]\n",
    "        return round(result, 2)\n",
    "\n",
    "# median, Q2\n",
    "def median(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]  # slice the indiced column\n",
    "        result = column.quantile([0.5]).values[0]  # compute the second quantile\n",
    "        return round(result, 2)\n",
    "\n",
    "# inner-quartile range\n",
    "def iqr(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        iqr = float(q3(filename, attribute_number))-float(q1(filename, attribute_number))\n",
    "        # return \"{:.2f}\".format(iqr)  # format to 2 decimal places.\n",
    "        return round(iqr, 2)\n",
    "\n",
    "# N (number of objects)\n",
    "def count(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]  # slice the indicated column\n",
    "        result = column.count()  # count \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56661eb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# OUTPUT TESTING\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mN:\u001b[39m\u001b[39m\"\u001b[39m, count(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m7\u001b[39;49m))  \u001b[39m# N\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQ1:\u001b[39m\u001b[39m\"\u001b[39m, q1(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m7\u001b[39m))  \u001b[39m# Q1\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmedian:\u001b[39m\u001b[39m\"\u001b[39m, median(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m7\u001b[39m))  \u001b[39m# Median (Q2)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 98\u001b[0m, in \u001b[0;36mcount\u001b[1;34m(filename, attribute_number)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     MC_telescope_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(filename, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     99\u001b[0m     MC_telescope_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(MC_telescope_data)\n\u001b[0;32m    100\u001b[0m     column \u001b[39m=\u001b[39m MC_telescope_df\u001b[39m.\u001b[39miloc[:,attribute_number]  \u001b[39m# slice the indicated column\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Nolan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# OUTPUT TESTING\n",
    "print(\"N:\", count(\"\", 7))  # N\n",
    "print(\"Q1:\", q1(\"\", 7))  # Q1\n",
    "print(\"median:\", median(\"\", 7))  # Median (Q2)\n",
    "print(\"Q3:\", q3(\"\", 7))  # Q3\n",
    "print(\"IQR:\", iqr(\"\", 7))  # IQR\n",
    "print(\"min:\", minimum(\"\", 7))  # min\n",
    "print(\"max:\", maximum(\"\", 7))  # max\n",
    "print(\"average:\", mean(\"\", 7))  # mean\n",
    "print(\"std. deviation:\", std(\"\", 7))  # standard deviation\n",
    "\n",
    "# # ERROR HANDLING\n",
    "# print(maximum(\"magic04.data\", 2.1))  # float (in-range)\n",
    "# print(minimum(\"magic04.data\", -1))  # out of bounds (less than)\n",
    "# print(mean(\"magic04.data\", 0))  # zero\n",
    "# print(std(\"magic04.data\", -1.5))  # negative out of range float\n",
    "# print(q1(\"magic04.data\", 10.5))  # positive out of range float\n",
    "# print(median(\"magic04.data\", 11))  # positive out of range (greater than)\n",
    "# print(q3(\"magic04.data\", -11.5))  # negative out of range float (less than)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b743b155",
   "metadata": {},
   "source": [
    "Next create a scatter plot using the 4th and 5th attributes. \n",
    "\n",
    "https://realpython.com/visualizing-python-plt-scatter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e941da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Attribute Definitions\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#     1.  fLength:  continuous  # major axis of ellipse [mm]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m#    10.  fDist:    continuous  # distance from origin to center of ellipse [mm]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m#    11.  class:    g,h         # gamma (signal), hadron (background)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m attribute_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mfLength\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfWidth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfSize\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfConc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfConc1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfAsym\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfM3Long\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfM3Trans\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfAlpha\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfDist\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "# Attribute Definitions\n",
    "#     1.  fLength:  continuous  # major axis of ellipse [mm]\n",
    "#     2.  fWidth:   continuous  # minor axis of ellipse [mm] \n",
    "#     3.  fSize:    continuous  # 10-log of sum of content of all pixels [in #phot]\n",
    "#     4.  fConc:    continuous  # ratio of sum of two highest pixels over fSize  [ratio]\n",
    "#     5.  fConc1:   continuous  # ratio of highest pixel over fSize  [ratio]\n",
    "#     6.  fAsym:    continuous  # distance from highest pixel to center, projected onto major axis [mm]\n",
    "#     7.  fM3Long:  continuous  # 3rd root of third moment along major axis  [mm] \n",
    "#     8.  fM3Trans: continuous  # 3rd root of third moment along minor axis  [mm]\n",
    "#     9.  fAlpha:   continuous  # angle of major axis with vector to origin [deg]\n",
    "#    10.  fDist:    continuous  # distance from origin to center of ellipse [mm]\n",
    "#    11.  class:    g,h         # gamma (signal), hadron (background)\n",
    "\n",
    "attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "MC_telescope_data = pd.read_csv(\"magic04.data\", header=None)  # tabular file header is absent\n",
    "MC_telescope_data.columns = attribute_names  # attach the column attribute labels\n",
    "MC_telescope_df = pd.DataFrame(MC_telescope_data)  # create a pandas dataframe \n",
    "\n",
    "\n",
    "MC_telescope_data_column_4 = MC_telescope_df.iloc[:,4]  #  \n",
    "MC_telescope_data_column_5 = MC_telescope_df.iloc[:,5]  # \n",
    "\n",
    "# KEEP THIS CODE FOR REFERENCE\n",
    "# plt.plot(MC_telescope_data_column_4, MC_telescope_data_column_5, \"o\")  # use plt.plot, if you want it to be fast.\n",
    "# plt.scatter(MC_telescope_data_column_4, MC_telescope_data_column_5)  # use plt.scatter, if you want more features.\n",
    "\n",
    "plt.scatter(x=MC_telescope_data_column_4, y=MC_telescope_data_column_5)  # use plt.scatter, if you want to use more features.\n",
    "plt.xlabel(attribute_names[3])  # \n",
    "plt.ylabel(attribute_names[4])  # \n",
    "plt.title(\"Scatter Plot of 4th and 5th Dimensions, MAGIC Telescope Data\")\n",
    "plt.savefig('scatter_plot_dimensions_4_5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1fa310",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70]\n",
    "interval_list = {'1-5':136, '6-15':181, '16-20':178, '21-50':695, '51-80':245, '81-110':177}  # Age:Frequency\n",
    "\n",
    "\n",
    "def lst_mean(lst):\n",
    "    result = np.mean(lst)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_median(lst):\n",
    "    result = np.median(lst)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_mode(lst):\n",
    "    result = stats.mode(lst, keepdims=True)\n",
    "    # return round(result, 3)\n",
    "    return result\n",
    "\n",
    "def lst_histogram(lst):\n",
    "    a = np.array(lst)\n",
    "    items = Counter(lst).keys()  # find the number of unique values in the set to set the bins\n",
    "    fig, ax = plt.subplots(figsize =(5, 3))\n",
    "    plt.hist(a, bins=range(np.min(lst)-5, np.max(lst)+5))\n",
    "\n",
    "def lst_multimode(lst):\n",
    "    res = []\n",
    "    test_list1 = Counter(lst)\n",
    "    temp = test_list1.most_common(1)[0][1]  # Extracts values of greatest frequency in the set.\n",
    "    for ele in lst:\n",
    "        if lst.count(ele) == temp:\n",
    "            res.append(ele)\n",
    "            res = list(set(res))\n",
    "    # printing results\n",
    "    return \"Data modality = \" + str(len(res)) + \"; mode list = \" + str(res)\n",
    "\n",
    "def lst_midrange(lst):\n",
    "    min = np.min(lst)\n",
    "    max = np.max(lst)\n",
    "    return round( ((min + max)/2.0), 3)\n",
    "\n",
    "def lst_q1(lst):\n",
    "    arr = np.array(lst)\n",
    "    result = np.quantile(arr, 0.25)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_q3(lst):\n",
    "    arr = np.array(lst)\n",
    "    result = np.quantile(arr, 0.75)\n",
    "    return round(result, 3)\n",
    "\n",
    "def cum_freq_list_from_inerval(dict):\n",
    "    cumulative_freq = 0\n",
    "    cumulative_freq_lst = []\n",
    "    for itm in dict:\n",
    "        cumulative_freq += dict[itm]  # collect the item frequency\n",
    "        cumulative_freq_lst.append(cumulative_freq)\n",
    "    n = cumulative_freq_lst[-1]\n",
    "    return cumulative_freq_lst\n",
    "    \n",
    "def find_n_by_2(dict):\n",
    "    cumulative_freq_lst = cum_freq_list_from_inerval(dict)\n",
    "    n = cumulative_freq_lst[-1]\n",
    "    n_by_2 = n/2\n",
    "    return n_by_2\n",
    "\n",
    "def find_median_bin(n_by_2, cumulative_freq_lst):\n",
    "    for i in range(len(cumulative_freq_lst)):\n",
    "        if n_by_2 <= cumulative_freq_lst[i]:\n",
    "            return i  # returns the index of the bin the median data point belongs to. \n",
    "        \n",
    "# def estimate_median(n_by_2, cumulative_freq_lst):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85845eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  29.963\n",
      "Q1:  20.5\n",
      "Q2:  25.0\n",
      "Q3:  35.0\n",
      "Data modality = 2; mode list = [25, 35]\n",
      "Midrange ((min+max)/2.0) :  41.5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMidrange ((min+max)/2.0) : \u001b[39m\u001b[39m\"\u001b[39m, lst_midrange(num_list))\n\u001b[0;32m     12\u001b[0m \u001b[39m# print(\"The data set is bimodal, with modes 25, 35.\")\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mprint\u001b[39m(lst_histogram(num_list))  \u001b[39m# use this to determine the mode(s) and modality of the set. \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mlst_histogram\u001b[1;34m(lst)\u001b[0m\n\u001b[0;32m     19\u001b[0m a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(lst)\n\u001b[0;32m     20\u001b[0m items \u001b[39m=\u001b[39m Counter(lst)\u001b[39m.\u001b[39mkeys()  \u001b[39m# find the number of unique values in the set to set the bins\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize \u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[0;32m     22\u001b[0m plt\u001b[39m.\u001b[39mhist(a, bins\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(np\u001b[39m.\u001b[39mmin(lst)\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m, np\u001b[39m.\u001b[39mmax(lst)\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# num_list = [13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70]\n",
    "# interval_list = {'1-5':136, '6-15':181, '16-20':178, '21-50':695, '51-80':245, '81-110':177}  # Age:Frequency\n",
    "\n",
    "# test the functions\n",
    "print(\"Mean: \", lst_mean(num_list))\n",
    "print(\"Q1: \", lst_q1(num_list))\n",
    "print(\"Q2: \", lst_median(num_list))\n",
    "print(\"Q3: \", lst_q3(num_list))\n",
    "print(lst_multimode(num_list))\n",
    "print(\"Midrange ((min+max)/2.0) : \", lst_midrange(num_list))\n",
    "\n",
    "# print(\"The data set is bimodal, with modes 25, 35.\")\n",
    "print(lst_histogram(num_list))  # use this to determine the mode(s) and modality of the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dc7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumm. frequency list: [136, 317, 495, 1190, 1435, 1612]\n",
      "n: 1612\n",
      "n/2: 806.0\n",
      "Group Index: 3\n",
      "Median data point resides in age group: '21-50'\n"
     ]
    }
   ],
   "source": [
    "# Question #4 on Computing Statistics using a Frequency Distribution Table\n",
    "# Estimated median = L + (L + ((n/2)-B)/G)*w\n",
    "# L is the lower class boundary of the group containing the median\n",
    "# n is the total number of values\n",
    "# B is the cumulative frequency of the groups before the median group\n",
    "# G is the frequency of the median group\n",
    "# w is the group width\n",
    "\n",
    "print(\"Cumm. frequency list:\", cum_freq_list_from_inerval(interval_list))\n",
    "print(\"n:\", cum_freq_list_from_inerval(interval_list)[-1])\n",
    "print(\"n/2:\", find_n_by_2(interval_list))\n",
    "print(\"Group Index:\", find_median_bin(find_n_by_2(interval_list), cum_freq_list_from_inerval(interval_list)))\n",
    "print(\"Median data point resides in age group: '21-50'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da1c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
