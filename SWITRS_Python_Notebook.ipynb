{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bed2bbc5-9d29-4c0b-812e-9fcbe775a582",
   "metadata": {},
   "source": [
    "<h1> Python Data Notebook (Template) </h1>\n",
    "<h3> Preserve this template, duplicate this file before using it!!</h3>\n",
    "<strong> Author: Micah Simmerman </strong>\n",
    "\n",
    "<strong>Resource URL:</strong>\n",
    "\n",
    "<strong>Database file(s), columnar dataset(s), etc.:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de39b907-6616-4c1f-8c75-8a020d020783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB file and Dependencies Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Libraries and packages\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "pd.__version__\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# SQLite db file connection is achieved in the try/except statements below. \n",
    "# TODO: add sections to read-in mySQL/PostGreSQL and csv files.\n",
    "\n",
    "db_file = 'California_Collision_Data.sqlite'  # name the db file name or resource file path.\n",
    "if os.path.isfile(db_file):  # determine if there is any SQLite db file by that name exists in the specified file location.\n",
    "    sqliteConnection = sqlite3.connect(db_file)  # establish a connection if the file does exist.\n",
    "    cursor = sqliteConnection.cursor()  # create a cursor object.\n",
    "    print('DB file and Dependencies Loaded.')  \n",
    "else:\n",
    "    print(\"No SQLite file detected.\")  # otherwise, print the connection status."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "327c63e2",
   "metadata": {},
   "source": [
    "Start by collecting information about the table names in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5d094e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 tables found.\n",
      "Table and attribute name extraction completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# This cell performs a search operation to collects variables used to extract data from the db file \n",
    "#  \n",
    "# This cell performs the following actions:\n",
    "# 1.) Extracts the name of each table in the sqlite db file. \n",
    "# 2.) Stores the attributes of each table in a 2D list by generating a list of attributes (see the inner for-loop below), \n",
    "#       creating a so called \"list-of-lists\". Each sublist corresponds to the same index as its' corresponding table  \n",
    "#       element in the \"table\" list.\n",
    "# Note: Each of the lists and sublists generated in this cell may be accessed by other routines in this notebook. \n",
    "\n",
    "\n",
    "# Collect some schema informatio concerning the tables in the database file.\n",
    "table_query = 'SELECT name from sqlite_master where type= \"table\"'\n",
    "tables = []  # GLOBAL TABLE INDEX\n",
    "attribute_list = []  # GLOBAL ATTRIBUTE INDEX\n",
    "\n",
    "try:\n",
    "    # the first part of this algorithm collects db schema information and inserts the name of each table into a global list.\n",
    "    cursor.execute(table_query)\n",
    "    result = cursor.fetchall()\n",
    "    print()\n",
    "    print(len(result), \"tables found.\")\n",
    "    for i in range(len(result)):\n",
    "        tables.append(result[i][0])\n",
    "        # print(result[i][0])\n",
    "\n",
    "    # the second part of the algorithm extracts the attributes of each table identified in the first step and places them into a 2D list. \n",
    "    for table in tables:\n",
    "        consumer_complaints_count_records = \"PRAGMA table_info(\"\n",
    "        consumer_complaints_count_records += str(table)  + \");\"  # build the sqlite query string using the current list object\n",
    "        cursor.execute(consumer_complaints_count_records)  # execute the query string\n",
    "        result = cursor.fetchall()  # collect the results\n",
    "        temp_list = []\n",
    "        for item in result:\n",
    "            temp_list.append(item[1])\n",
    "        attribute_list.append(temp_list)  # export the list attribute table after it is built\n",
    "        # print(\"The\", table, \"table contains\", result[0][0], \"data points\")  # print the results\n",
    "        # print()\n",
    "    print(\"Table and attribute name extraction completed successfully.\")\n",
    "except:\n",
    "    print(\"Error:no db connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fec84088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:  ['case_ids', 'collisions', 'victims', 'parties']\n",
      "Attribute list:  [['case_id', 'db_year'], ['case_id', 'jurisdiction', 'officer_id', 'reporting_district', 'chp_shift', 'population', 'county_city_location', 'county_location', 'special_condition', 'beat_type', 'chp_beat_type', 'city_division_lapd', 'chp_beat_class', 'beat_number', 'primary_road', 'secondary_road', 'distance', 'direction', 'intersection', 'weather_1', 'weather_2', 'state_highway_indicator', 'caltrans_county', 'caltrans_district', 'state_route', 'route_suffix', 'postmile_prefix', 'postmile', 'location_type', 'ramp_intersection', 'side_of_highway', 'tow_away', 'collision_severity', 'killed_victims', 'injured_victims', 'party_count', 'primary_collision_factor', 'pcf_violation_code', 'pcf_violation_category', 'pcf_violation', 'pcf_violation_subsection', 'hit_and_run', 'type_of_collision', 'motor_vehicle_involved_with', 'pedestrian_action', 'road_surface', 'road_condition_1', 'road_condition_2', 'lighting', 'control_device', 'chp_road_type', 'pedestrian_collision', 'bicycle_collision', 'motorcycle_collision', 'truck_collision', 'not_private_property', 'alcohol_involved', 'statewide_vehicle_type_at_fault', 'chp_vehicle_type_at_fault', 'severe_injury_count', 'other_visible_injury_count', 'complaint_of_pain_injury_count', 'pedestrian_killed_count', 'pedestrian_injured_count', 'bicyclist_killed_count', 'bicyclist_injured_count', 'motorcyclist_killed_count', 'motorcyclist_injured_count', 'primary_ramp', 'secondary_ramp', 'latitude', 'longitude', 'collision_date', 'collision_time', 'process_date'], ['id', 'case_id', 'party_number', 'victim_role', 'victim_sex', 'victim_age', 'victim_degree_of_injury', 'victim_seating_position', 'victim_safety_equipment_1', 'victim_safety_equipment_2', 'victim_ejected'], ['id', 'case_id', 'party_number', 'party_type', 'at_fault', 'party_sex', 'party_age', 'party_sobriety', 'party_drug_physical', 'direction_of_travel', 'party_safety_equipment_1', 'party_safety_equipment_2', 'financial_responsibility', 'hazardous_materials', 'cellphone_in_use', 'cellphone_use_type', 'school_bus_related', 'oaf_violation_code', 'oaf_violation_category', 'oaf_violation_section', 'oaf_violation_suffix', 'other_associate_factor_1', 'other_associate_factor_2', 'party_number_killed', 'party_number_injured', 'movement_preceding_collision', 'vehicle_year', 'vehicle_make', 'statewide_vehicle_type', 'chp_vehicle_type_towing', 'chp_vehicle_type_towed', 'party_race']]\n"
     ]
    }
   ],
   "source": [
    "# Now we have a list of tables and their respective attribute columns.\n",
    "print(\"Tables in the database: \", tables)\n",
    "print(\"Attribute list: \", attribute_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7063affc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The case_ids table contains 9424334 data points.\n",
      "The collisions table contains 9424334 data points.\n",
      "The victims table contains 9639334 data points.\n",
      "The parties table contains 18669166 data points.\n"
     ]
    }
   ],
   "source": [
    "# This cell will determine the number of data points within each table of the database and enumerate them one by one.\n",
    "\n",
    "# attribute_list = []\n",
    "# count_records = 'SELECT COUNT(*) FROM ('  # start the string.\n",
    "# count_records += table + \");\"  # concatenate the variable and complete the string build.\n",
    "# cursor.execute(count_records)  # \n",
    "# result = cursor.fetchall()\n",
    "\n",
    "for table in range(len(tables)):\n",
    "    \n",
    "    count_records = 'SELECT COUNT(*) FROM ('  # start the string.\n",
    "    count_records += tables[table] + \");\"  # concatenate the variable and complete the string build.\n",
    "    cursor.execute(count_records)  # \n",
    "    result = cursor.fetchall()\n",
    "    print(\"The\", tables[table], \"table contains\", result[0][0], \"data points\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a5fc3c6",
   "metadata": {},
   "source": [
    "TO DO: INSERT BASIC STATISTICS FUNCTIONS BETWEEN THESE TWO CELLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will defines a first round Data Cube Function(s)\n",
    "create_data_cube = \"\"\"SELECT \n",
    "    c1, c2, AGGREGATE_FUNCTION(c3)\n",
    "FROM\n",
    "    table_name\n",
    "GROUP BY CUBE(c1 , c2);\"\"\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a666f304",
   "metadata": {},
   "source": [
    "The functions below currently only work on csv files. Add a conditional behavior to these functions and extend them to extract equivalent data from a table in an SQLite db file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are basic statistic functions that can be used as helper routines in longer extraction/cleaning/clustering efforts. \n",
    "# The remainder of the notebook contains code written to examine the telescope data. You can to generalize each function or create a\n",
    "# python scrip to examine tabular data files in a terminal window.\n",
    "\n",
    "# There are many methods that you can use to extract SQLite table data into a pandas dataframe or a numpy data object to generate graphics, \n",
    "# or perform package-based statistical testing. At this point in the notebook (and as of 11-June-2023).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# maximum value (max)\n",
    "def maximum(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)  # read the file into a pandas dataframe\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]  # project the indicated column from the pandas dataframe\n",
    "        result = column.max()  # find the maximum value\n",
    "        return round(result, 2)\n",
    "\n",
    "# minimum value (min)\n",
    "def minimum(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.min()  # find the minimum value\n",
    "        return round(result, 2)\n",
    "\n",
    "# average (mean)\n",
    "def mean(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.mean()  # calculate the average value\n",
    "        return round(result, 2)\n",
    "\n",
    "# standard deviation\n",
    "def std(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        column = MC_telescope_data.iloc[:,attribute_number]\n",
    "        result = column.std()  # calculate standard deviation\n",
    "        return round(result, 2)\n",
    "\n",
    "# first quantile, Q1\n",
    "def q1(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]\n",
    "        result = column.quantile([0.25]).values[0]\n",
    "        return round(result, 2)\n",
    "\n",
    "# third quantile, Q3\n",
    "def q3(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]\n",
    "        result = column.quantile([0.75]).values[0]\n",
    "        return round(result, 2)\n",
    "\n",
    "# median, Q2\n",
    "def median(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]  # slice the indiced column\n",
    "        result = column.quantile([0.5]).values[0]  # compute the second quantile\n",
    "        return round(result, 2)\n",
    "\n",
    "# inner-quartile range\n",
    "def iqr(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        iqr = float(q3(filename, attribute_number))-float(q1(filename, attribute_number))\n",
    "        # return \"{:.2f}\".format(iqr)  # format to 2 decimal places.\n",
    "        return round(iqr, 2)\n",
    "\n",
    "# N (number of objects)\n",
    "def count(filename, attribute_number):\n",
    "    if attribute_number < 1 or attribute_number > 10 or attribute_number%1!=0:\n",
    "        return None\n",
    "    else:\n",
    "        MC_telescope_data = pd.read_csv(filename, header=None)\n",
    "        MC_telescope_df = pd.DataFrame(MC_telescope_data)\n",
    "        column = MC_telescope_df.iloc[:,attribute_number]  # slice the indicated column\n",
    "        result = column.count()  # count \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56661eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT TESTING\n",
    "print(\"N:\", count(\"\", 7))  # N\n",
    "print(\"Q1:\", q1(\"\", 7))  # Q1\n",
    "print(\"median:\", median(\"\", 7))  # Median (Q2)\n",
    "print(\"Q3:\", q3(\"\", 7))  # Q3\n",
    "print(\"IQR:\", iqr(\"\", 7))  # IQR\n",
    "print(\"min:\", minimum(\"\", 7))  # min\n",
    "print(\"max:\", maximum(\"\", 7))  # max\n",
    "print(\"average:\", mean(\"\", 7))  # mean\n",
    "print(\"std. deviation:\", std(\"\", 7))  # standard deviation\n",
    "\n",
    "# # ERROR HANDLING\n",
    "# print(maximum(\"magic04.data\", 2.1))  # float (in-range)\n",
    "# print(minimum(\"magic04.data\", -1))  # out of bounds (less than)\n",
    "# print(mean(\"magic04.data\", 0))  # zero\n",
    "# print(std(\"magic04.data\", -1.5))  # negative out of range float\n",
    "# print(q1(\"magic04.data\", 10.5))  # positive out of range float\n",
    "# print(median(\"magic04.data\", 11))  # positive out of range (greater than)\n",
    "# print(q3(\"magic04.data\", -11.5))  # negative out of range float (less than)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b743b155",
   "metadata": {},
   "source": [
    "Next create a scatter plot using the 4th and 5th attributes. \n",
    "\n",
    "https://realpython.com/visualizing-python-plt-scatter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e941da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "# Attribute Definitions\n",
    "#     1.  fLength:  continuous  # major axis of ellipse [mm]\n",
    "#     2.  fWidth:   continuous  # minor axis of ellipse [mm] \n",
    "#     3.  fSize:    continuous  # 10-log of sum of content of all pixels [in #phot]\n",
    "#     4.  fConc:    continuous  # ratio of sum of two highest pixels over fSize  [ratio]\n",
    "#     5.  fConc1:   continuous  # ratio of highest pixel over fSize  [ratio]\n",
    "#     6.  fAsym:    continuous  # distance from highest pixel to center, projected onto major axis [mm]\n",
    "#     7.  fM3Long:  continuous  # 3rd root of third moment along major axis  [mm] \n",
    "#     8.  fM3Trans: continuous  # 3rd root of third moment along minor axis  [mm]\n",
    "#     9.  fAlpha:   continuous  # angle of major axis with vector to origin [deg]\n",
    "#    10.  fDist:    continuous  # distance from origin to center of ellipse [mm]\n",
    "#    11.  class:    g,h         # gamma (signal), hadron (background)\n",
    "\n",
    "attribute_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "MC_telescope_data = pd.read_csv(\"magic04.data\", header=None)  # tabular file header is absent\n",
    "MC_telescope_data.columns = attribute_names  # attach the column attribute labels\n",
    "MC_telescope_df = pd.DataFrame(MC_telescope_data)  # create a pandas dataframe \n",
    "\n",
    "\n",
    "MC_telescope_data_column_4 = MC_telescope_df.iloc[:,4]  #  \n",
    "MC_telescope_data_column_5 = MC_telescope_df.iloc[:,5]  # \n",
    "\n",
    "# KEEP THIS CODE FOR REFERENCE\n",
    "# plt.plot(MC_telescope_data_column_4, MC_telescope_data_column_5, \"o\")  # use plt.plot, if you want it to be fast.\n",
    "# plt.scatter(MC_telescope_data_column_4, MC_telescope_data_column_5)  # use plt.scatter, if you want more features.\n",
    "\n",
    "plt.scatter(x=MC_telescope_data_column_4, y=MC_telescope_data_column_5)  # use plt.scatter, if you want to use more features.\n",
    "plt.xlabel(attribute_names[3])  # \n",
    "plt.ylabel(attribute_names[4])  # \n",
    "plt.title(\"Scatter Plot of 4th and 5th Dimensions, MAGIC Telescope Data\")\n",
    "plt.savefig('scatter_plot_dimensions_4_5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1fa310",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70]\n",
    "interval_list = {'1-5':136, '6-15':181, '16-20':178, '21-50':695, '51-80':245, '81-110':177}  # Age:Frequency\n",
    "\n",
    "\n",
    "def lst_mean(lst):\n",
    "    result = np.mean(lst)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_median(lst):\n",
    "    result = np.median(lst)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_mode(lst):\n",
    "    result = stats.mode(lst, keepdims=True)\n",
    "    # return round(result, 3)\n",
    "    return result\n",
    "\n",
    "def lst_histogram(lst):\n",
    "    a = np.array(lst)\n",
    "    items = Counter(lst).keys()  # find the number of unique values in the set to set the bins\n",
    "    fig, ax = plt.subplots(figsize =(5, 3))\n",
    "    plt.hist(a, bins=range(np.min(lst)-5, np.max(lst)+5))\n",
    "\n",
    "def lst_multimode(lst):\n",
    "    res = []\n",
    "    test_list1 = Counter(lst)\n",
    "    temp = test_list1.most_common(1)[0][1]  # Extracts values of greatest frequency in the set.\n",
    "    for ele in lst:\n",
    "        if lst.count(ele) == temp:\n",
    "            res.append(ele)\n",
    "            res = list(set(res))\n",
    "    # printing results\n",
    "    return \"Data modality = \" + str(len(res)) + \"; mode list = \" + str(res)\n",
    "\n",
    "def lst_midrange(lst):\n",
    "    min = np.min(lst)\n",
    "    max = np.max(lst)\n",
    "    return round( ((min + max)/2.0), 3)\n",
    "\n",
    "def lst_q1(lst):\n",
    "    arr = np.array(lst)\n",
    "    result = np.quantile(arr, 0.25)\n",
    "    return round(result, 3)\n",
    "\n",
    "def lst_q3(lst):\n",
    "    arr = np.array(lst)\n",
    "    result = np.quantile(arr, 0.75)\n",
    "    return round(result, 3)\n",
    "\n",
    "def cum_freq_list_from_inerval(dict):\n",
    "    cumulative_freq = 0\n",
    "    cumulative_freq_lst = []\n",
    "    for itm in dict:\n",
    "        cumulative_freq += dict[itm]  # collect the item frequency\n",
    "        cumulative_freq_lst.append(cumulative_freq)\n",
    "    n = cumulative_freq_lst[-1]\n",
    "    return cumulative_freq_lst\n",
    "    \n",
    "def find_n_by_2(dict):\n",
    "    cumulative_freq_lst = cum_freq_list_from_inerval(dict)\n",
    "    n = cumulative_freq_lst[-1]\n",
    "    n_by_2 = n/2\n",
    "    return n_by_2\n",
    "\n",
    "def find_median_bin(n_by_2, cumulative_freq_lst):\n",
    "    for i in range(len(cumulative_freq_lst)):\n",
    "        if n_by_2 <= cumulative_freq_lst[i]:\n",
    "            return i  # returns the index of the bin the median data point belongs to. \n",
    "        \n",
    "# def estimate_median(n_by_2, cumulative_freq_lst):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85845eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_list = [13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70]\n",
    "# interval_list = {'1-5':136, '6-15':181, '16-20':178, '21-50':695, '51-80':245, '81-110':177}  # Age:Frequency\n",
    "\n",
    "# test the functions\n",
    "print(\"Mean: \", lst_mean(num_list))\n",
    "print(\"Q1: \", lst_q1(num_list))\n",
    "print(\"Q2: \", lst_median(num_list))\n",
    "print(\"Q3: \", lst_q3(num_list))\n",
    "print(lst_multimode(num_list))\n",
    "print(\"Midrange ((min+max)/2.0) : \", lst_midrange(num_list))\n",
    "\n",
    "# print(\"The data set is bimodal, with modes 25, 35.\")\n",
    "print(lst_histogram(num_list))  # use this to determine the mode(s) and modality of the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question #4 on Computing Statistics using a Frequency Distribution Table\n",
    "# Estimated median = L + (L + ((n/2)-B)/G)*w\n",
    "# L is the lower class boundary of the group containing the median\n",
    "# n is the total number of values\n",
    "# B is the cumulative frequency of the groups before the median group\n",
    "# G is the frequency of the median group\n",
    "# w is the group width\n",
    "\n",
    "print(\"Cumm. frequency list:\", cum_freq_list_from_inerval(interval_list))\n",
    "print(\"n:\", cum_freq_list_from_inerval(interval_list)[-1])\n",
    "print(\"n/2:\", find_n_by_2(interval_list))\n",
    "print(\"Group Index:\", find_median_bin(find_n_by_2(interval_list), cum_freq_list_from_inerval(interval_list)))\n",
    "print(\"Median data point resides in age group: '21-50'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
